{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675fc0ee",
   "metadata": {},
   "source": [
    "# U-Net Model for Image Segmentation\n",
    "\n",
    "This notebook implements a U-Net model for image segmentation using PyTorch. It includes:\n",
    "- U-Net model architecture.\n",
    "- Dataset preparation with preprocessing.\n",
    "- Training and evaluation functions.\n",
    "- Utility functions like Dice coefficient and loss.\n",
    "\n",
    "Let's start by importing the necessary libraries and defining the U-Net architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from os import listdir\n",
    "from os.path import splitext, isfile, join\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f21563",
   "metadata": {},
   "source": [
    "## Dataloader Class\n",
    "\n",
    "The `Dataloader` class is responsible for loading images and their corresponding masks from specified directories. It preprocesses the images and masks to ensure they are in the correct format and size for training the U-Net model. The class inherits from `torch.utils.data.Dataset`, allowing it to be used seamlessly with PyTorch's data loading utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f682fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename):\n",
    "    ext = splitext(filename)[1]\n",
    "    if ext == '.npy':\n",
    "        return Image.fromarray(np.load(filename))\n",
    "    elif ext in ['.pt', '.pth']:\n",
    "        return Image.fromarray(torch.load(filename).numpy())\n",
    "    else:\n",
    "        return Image.open(filename)\n",
    "\n",
    "\n",
    "def unique_mask_values(idx, mask_dir, mask_suffix):\n",
    "    mask_file = list(mask_dir.glob(idx + mask_suffix + '.*'))[0]\n",
    "    mask = np.asarray(load_image(mask_file))\n",
    "    if mask.ndim == 2:\n",
    "        return np.unique(mask)\n",
    "    elif mask.ndim == 3:\n",
    "        mask = mask.reshape(-1, mask.shape[-1])\n",
    "        return np.unique(mask, axis=0)\n",
    "    else:\n",
    "        raise ValueError(f'Loaded masks should have 2 or 3 dimensions, found {mask.ndim}')\n",
    "\n",
    "\n",
    "class Dataloader(Dataset):\n",
    "    def __init__(self, images_dir: str, mask_dir: str, scale: float = 1.0, mask_suffix: str = ''):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
    "        self.scale = scale\n",
    "        self.mask_suffix = mask_suffix\n",
    "\n",
    "        self.ids = [splitext(file)[0] for file in listdir(images_dir) if isfile(join(images_dir, file)) and not file.startswith('.')]\n",
    "        if not self.ids:\n",
    "            raise RuntimeError(f'No input file found in {images_dir}, make sure you put your images there')\n",
    "\n",
    "        print(f'Creating dataset with {len(self.ids)} examples')\n",
    "        print('Scanning mask files to determine unique values')\n",
    "        with Pool() as p:\n",
    "            unique = list(tqdm(\n",
    "                p.imap(partial(unique_mask_values, mask_dir=self.mask_dir, mask_suffix=self.mask_suffix), self.ids),\n",
    "                total=len(self.ids)\n",
    "            ))\n",
    "\n",
    "        self.mask_values = list(sorted(np.unique(np.concatenate(unique), axis=0).tolist()))\n",
    "        print(f'Unique mask values: {self.mask_values}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(mask_values, pil_img, scale, is_mask):\n",
    "        w, h = pil_img.size\n",
    "        newW, newH = int(scale * w), int(scale * h)\n",
    "        assert newW > 0 and newH > 0, 'Scale is too small, resized images would have no pixel'\n",
    "        pil_img = pil_img.resize((newW, newH), resample=Image.NEAREST if is_mask else Image.BICUBIC)\n",
    "        img = np.asarray(pil_img)\n",
    "\n",
    "        if is_mask:\n",
    "            mask = np.zeros((newH, newW), dtype=np.int64)\n",
    "            for i, v in enumerate(mask_values):\n",
    "                if img.ndim == 2:\n",
    "                    mask[img == v] = i\n",
    "                else:\n",
    "                    mask[(img == v).all(-1)] = i\n",
    "\n",
    "            return mask\n",
    "\n",
    "        else:\n",
    "            if img.ndim == 2:\n",
    "                img = img[np.newaxis, ...]\n",
    "            else:\n",
    "                img = img.transpose((2, 0, 1))\n",
    "\n",
    "            if (img > 1).any():\n",
    "                img = img / 255.0\n",
    "\n",
    "            return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.ids[idx]\n",
    "        mask_file = list(self.mask_dir.glob(name + self.mask_suffix + '.*'))\n",
    "        img_file = list(self.images_dir.glob(name + '.*'))\n",
    "\n",
    "        assert len(img_file) == 1, f'Either no image or multiple images found for the ID {name}: {img_file}'\n",
    "        assert len(mask_file) == 1, f'Either no mask or multiple masks found for the ID {name}: {mask_file}'\n",
    "        mask = load_image(mask_file[0])\n",
    "        img = load_image(img_file[0])\n",
    "\n",
    "        assert img.size == mask.size, \\\n",
    "            f'Image and mask {name} should be the same size, but are {img.size} and {mask.size}'\n",
    "\n",
    "        img = self.preprocess(self.mask_values, img, self.scale, is_mask=False)\n",
    "        mask = self.preprocess(self.mask_values, mask, self.scale, is_mask=True)\n",
    "\n",
    "        return {\n",
    "            'image': torch.as_tensor(img.copy()).float().contiguous(),\n",
    "            'mask': torch.as_tensor(mask.copy()).long().contiguous()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987a5be",
   "metadata": {},
   "source": [
    "## DoubleConv Class\n",
    "\n",
    "This class defines a sequence of two convolution layers, each followed by batch normalization and a ReLU activation. This structure is commonly used in U-Net blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3964a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcbcc1d",
   "metadata": {},
   "source": [
    "## Down Class\n",
    "\n",
    "This class defines the downsampling part of the U-Net. It includes a max pooling operation followed by a `DoubleConv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70553a1a",
   "metadata": {},
   "source": [
    "## Up Class\n",
    "\n",
    "This class handles the upsampling and the concatenation of feature maps from the encoder path. It allows using either bilinear interpolation or transposed convolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # Concatenate along the channel dimension\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b9d92",
   "metadata": {},
   "source": [
    "## OutConv Class\n",
    "\n",
    "This class defines the final convolutional layer, which maps the output features to the desired number of classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6663d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de3b283",
   "metadata": {},
   "source": [
    "## UNet Class\n",
    "\n",
    "The main U-Net class combines the previously defined layers to create the full U-Net architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        \"\"\"Enables gradient checkpointing to save memory during training\"\"\"\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17300ea9",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "The Dice coefficient measures the overlap between the predicted segmentation and the ground truth, while the Dice loss is used as a loss function to optimize the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ab587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(input: torch.Tensor, target: torch.Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    assert input.size() == target.size()\n",
    "    assert input.dim() == 3 or not reduce_batch_first\n",
    "\n",
    "    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n",
    "\n",
    "    inter = 2 * (input * target).sum(dim=sum_dim)\n",
    "    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n",
    "    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
    "\n",
    "    dice = (inter + epsilon) / (sets_sum + epsilon)\n",
    "    return dice.mean()\n",
    "\n",
    "\n",
    "def multiclass_dice_coeff(input: torch.Tensor, target: torch.Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all classes\n",
    "    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n",
    "\n",
    "\n",
    "def dice_loss(input: torch.Tensor, target: torch.Tensor, multiclass: bool = False):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
    "    return 1 - fn(input, target, reduce_batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a27d9",
   "metadata": {},
   "source": [
    "## Evaluation function\n",
    "\n",
    "The evaluation function computes the Dice score, which measures the overlap between the predicted segmentation masks and the ground truth masks. It iterates over the validation dataset, making predictions for each image and comparing them to the true masks. The function handles both binary and multiclass segmentation tasks, converting the predicted masks to one-hot format when necessary. The final Dice score is averaged over all validation batches to provide a measure of the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(net, dataloader, device, amp):\n",
    "    net.eval()\n",
    "    num_val_batches = len(dataloader)\n",
    "    dice_score = 0\n",
    "\n",
    "    # iterate over the validation set\n",
    "    with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
    "        for batch in tqdm(dataloader, total=num_val_batches, desc='Validation round', unit='batch', leave=False):\n",
    "            image, mask_true = batch['image'], batch['mask']\n",
    "\n",
    "            # move images and labels to correct device and type\n",
    "            image = image.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "            mask_true = mask_true.to(device=device, dtype=torch.long)\n",
    "\n",
    "            # predict the mask\n",
    "            mask_pred = net(image)\n",
    "\n",
    "            if net.n_classes == 1:\n",
    "                assert mask_true.min() >= 0 and mask_true.max() <= 1, 'True mask indices should be in [0, 1]'\n",
    "                mask_pred = (F.sigmoid(mask_pred) > 0.5).float()\n",
    "                # compute the Dice score\n",
    "                dice_score += dice_coeff(mask_pred, mask_true, reduce_batch_first=False)\n",
    "            else:\n",
    "                assert mask_true.min() >= 0 and mask_true.max() < net.n_classes, 'True mask indices should be in [0, n_classes['\n",
    "                # convert to one-hot format\n",
    "                mask_true = F.one_hot(mask_true, net.n_classes).permute(0, 3, 1, 2).float()\n",
    "                mask_pred = F.one_hot(mask_pred.argmax(dim=1), net.n_classes).permute(0, 3, 1, 2).float()\n",
    "                # compute the Dice score, ignoring background\n",
    "                dice_score += multiclass_dice_coeff(mask_pred[:, 1:], mask_true[:, 1:], reduce_batch_first=False)\n",
    "\n",
    "    net.train()\n",
    "    return dice_score / max(num_val_batches, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783d326c",
   "metadata": {},
   "source": [
    "Defining dir path for images, masks, and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9337c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory paths for images, masks, and model checkpoints.\n",
    "# These paths are used to load the training images and their corresponding masks,\n",
    "# as well as to save the model checkpoints during training.\n",
    "dir_img = Path('./data/imgs/')  # Path to the directory containing input images\n",
    "dir_mask = Path('./data/masks/')  # Path to the directory containing ground truth masks\n",
    "dir_checkpoint = Path('./checkpoints/')  # Path to the directory where model checkpoints will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18df88e",
   "metadata": {},
   "source": [
    "## Training function\n",
    "\n",
    "The `train_model` function is responsible for training the U-Net model on the provided dataset. It takes several parameters to configure the training process, including the model, device, number of epochs, batch size, learning rate, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        device,\n",
    "        epochs: int = 5,\n",
    "        batch_size: int = 1,\n",
    "        learning_rate: float = 1e-5,\n",
    "        val_percent: float = 0.1,\n",
    "        save_checkpoint: bool = True,\n",
    "        img_scale: float = 0.5,\n",
    "        amp: bool = False,\n",
    "        weight_decay: float = 1e-8,\n",
    "        momentum: float = 0.999,\n",
    "        gradient_clipping: float = 1.0,\n",
    "):\n",
    "    # 1. Create dataset\n",
    "    dataset = Dataloader(dir_img, dir_mask, img_scale)\n",
    "\n",
    "    # 2. Split into train / validation partitions\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    # 3. Create data loaders\n",
    "    loader_args = dict(batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True)\n",
    "    train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "\n",
    "\n",
    "    print(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {learning_rate}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_checkpoint}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {img_scale}\n",
    "        Mixed Precision: {amp}\n",
    "    ''')\n",
    "\n",
    "    # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "    optimizer = optim.RMSprop(model.parameters(),\n",
    "                              lr=learning_rate, weight_decay=weight_decay, momentum=momentum, foreach=True)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)  # goal: maximize Dice score\n",
    "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    criterion = nn.CrossEntropyLoss() if model.n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "    global_step = 0\n",
    "\n",
    "    # 5. Begin training\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                images, true_masks = batch['image'], batch['mask']\n",
    "\n",
    "                assert images.shape[1] == model.n_channels, \\\n",
    "                    f'Network has been defined with {model.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "                true_masks = true_masks.to(device=device, dtype=torch.long)\n",
    "\n",
    "                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
    "                    masks_pred = model(images)\n",
    "                    if model.n_classes == 1:\n",
    "                        loss = criterion(masks_pred.squeeze(1), true_masks.float())\n",
    "                        loss += dice_loss(F.sigmoid(masks_pred.squeeze(1)), true_masks.float(), multiclass=False)\n",
    "                    else:\n",
    "                        loss = criterion(masks_pred, true_masks)\n",
    "                        loss += dice_loss(\n",
    "                            F.softmax(masks_pred, dim=1).float(),\n",
    "                            F.one_hot(true_masks, model.n_classes).permute(0, 3, 1, 2).float(),\n",
    "                            multiclass=True\n",
    "                        )\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                grad_scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "\n",
    "                pbar.update(images.shape[0])\n",
    "                global_step += 1\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                # Evaluation round\n",
    "                division_step = (n_train // (5 * batch_size))\n",
    "                if division_step > 0:\n",
    "                    if global_step % division_step == 0:\n",
    "                        histograms = {}\n",
    "                        for tag, value in model.named_parameters():\n",
    "                            tag = tag.replace('/', '.')\n",
    "                            if not (torch.isinf(value) | torch.isnan(value)).any():\n",
    "                                histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
    "                            if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():\n",
    "                                histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
    "\n",
    "                        val_score = evaluate(model, val_loader, device, amp)\n",
    "                        scheduler.step(val_score)\n",
    "\n",
    "                        print('Validation Dice score: {}'.format(val_score))\n",
    "\n",
    "        if save_checkpoint:\n",
    "            Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)\n",
    "            state_dict = model.state_dict()\n",
    "            state_dict['mask_values'] = dataset.mask_values\n",
    "            torch.save(state_dict, str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch)))\n",
    "            print(f'Checkpoint {epoch} saved!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505cef15",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4da0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 1\n",
    "learning_rate = 1e-5\n",
    "load = None\n",
    "scale = 0.5\n",
    "val = 10.0\n",
    "amp = False\n",
    "bilinear = False\n",
    "classes = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')\n",
    "\n",
    "# Change here to adapt to your data\n",
    "# n_channels=3 for RGB images\n",
    "# n_classes is the number of probabilities you want to get per pixel\n",
    "model = UNet(n_channels=3, n_classes=classes, bilinear=bilinear)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "print(f'Network:\\n\\t{model.n_channels} input channels\\n\\t{model.n_classes} output channels (classes)\\n\\t{\"Bilinear\" if model.bilinear else \"Transposed conv\"} upscaling')\n",
    "\n",
    "if load:\n",
    "    state_dict = torch.load(load, map_location=device)\n",
    "    del state_dict['mask_values']\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f'Model loaded from {load}')\n",
    "\n",
    "model.to(device=device)\n",
    "torch.cuda.empty_cache()\n",
    "model.use_checkpointing()\n",
    "train_model(\n",
    "    model=model,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    device=device,\n",
    "    img_scale=scale,\n",
    "    val_percent=val / 100,\n",
    "    amp=amp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edff908",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(net,\n",
    "                full_img,\n",
    "                device,\n",
    "                scale_factor=1,\n",
    "                out_threshold=0.5):\n",
    "    net.eval()\n",
    "    img = torch.from_numpy(Dataloader.preprocess(None, full_img, scale_factor, is_mask=False))\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = net(img).cpu()\n",
    "        output = F.interpolate(output, (full_img.size[1], full_img.size[0]), mode='bilinear')\n",
    "        if net.n_classes > 1:\n",
    "            mask = output.argmax(dim=1)\n",
    "        else:\n",
    "            mask = torch.sigmoid(output) > out_threshold\n",
    "\n",
    "    return mask[0].long().squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_image(mask: np.ndarray, mask_values):\n",
    "    if isinstance(mask_values[0], list):\n",
    "        out = np.zeros((mask.shape[-2], mask.shape[-1], len(mask_values[0])), dtype=np.uint8)\n",
    "    elif mask_values == [0, 1]:\n",
    "        out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=bool)\n",
    "    else:\n",
    "        out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=np.uint8)\n",
    "\n",
    "    if mask.ndim == 3:\n",
    "        mask = np.argmax(mask, axis=0)\n",
    "\n",
    "    for i, v in enumerate(mask_values):\n",
    "        out[mask == i] = v\n",
    "\n",
    "    return Image.fromarray(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_and_mask(img, mask):\n",
    "    classes = mask.max() + 1\n",
    "    fig, ax = plt.subplots(1, classes + 1)\n",
    "    ax[0].set_title('Input image')\n",
    "    ax[0].imshow(img)\n",
    "    for i in range(classes):\n",
    "        ax[i + 1].set_title(f'Mask (class {i + 1})')\n",
    "        ax[i + 1].imshow(mask == i)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3953270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model_path, input_file, out_file, scale=0.5, mask_threshold=0.5, classes=2, bilinear=False):\n",
    "    net = UNet(n_channels=3, n_classes=classes, bilinear=bilinear)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Loading model {model_path}')\n",
    "    print(f'Using device {device}')\n",
    "\n",
    "    net.to(device=device)\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    mask_values = state_dict.pop('mask_values', [0, 1])\n",
    "    net.load_state_dict(state_dict)\n",
    "\n",
    "    print('Model loaded!')\n",
    "\n",
    "    print(f'Predicting image {input_file} ...')\n",
    "    img = Image.open(input_file)\n",
    "\n",
    "    mask = predict_img(net=net,\n",
    "                        full_img=img,\n",
    "                        scale_factor=scale,\n",
    "                        out_threshold=mask_threshold,\n",
    "                        device=device)\n",
    "\n",
    "    result = mask_to_image(mask, mask_values)\n",
    "    result.save(out_file)\n",
    "    print(f'Mask saved to {out_file}')\n",
    "\n",
    "    print(f'Visualizing results for image {input_file}, close to continue...')\n",
    "    plot_img_and_mask(img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51831e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'MODEL.pth'\n",
    "input_file = 'data/test_images/1.png'\n",
    "out_file = 'data/test_images/1_OUT.png'\n",
    "mask_threshold = 0.5\n",
    "\n",
    "inference(model, input_file, out_file, scale, mask_threshold, classes, bilinear)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
