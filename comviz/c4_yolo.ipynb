{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94322e18",
   "metadata": {},
   "source": [
    "\n",
    "# YOLOv8 Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72282d55",
   "metadata": {},
   "source": [
    "Download a mini COCO2017 for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mdaniyalk/workshop-datains-oct-2024/refs/heads/main/comviz/coco-class.yaml -O coco-class.yaml\n",
    "\n",
    "!wget https://huggingface.co/datasets/bryanbocao/coco_minitrain/resolve/main/coco_minitrain_25k.zip -O coco_minitrain_25k.zip\n",
    "!unzip -q coco_minitrain_25k.zip -d ./COCO\n",
    "!rm coco_minitrain_25k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51fa1d",
   "metadata": {},
   "source": [
    "\n",
    "## Imports and Initialization\n",
    "The necessary libraries are imported, and we disable warnings for cleaner output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import argparse\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch.utils import data\n",
    "from torch.nn.functional import cross_entropy, one_hot\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5857c",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset Utility Functions\n",
    "The `Dataset` class is used to manage and preprocess image data for training. It includes methods for loading individual images, applying augmentations, and combining samples into batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62957529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, filenames, input_size, params, augment):\n",
    "        self.params = params\n",
    "        self.mosaic = augment\n",
    "        self.augment = augment\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Read labels\n",
    "        cache = self.load_label(filenames)\n",
    "        labels, shapes = zip(*cache.values())\n",
    "        self.labels = list(labels)\n",
    "        self.shapes = np.array(shapes, dtype=np.float64)\n",
    "        self.filenames = list(cache.keys())  # update\n",
    "        self.n = len(shapes)  # number of samples\n",
    "        self.indices = range(self.n)\n",
    "        # Albumentations (optional, only used if package is installed)\n",
    "        self.albumentations = Albumentations()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.indices[index]\n",
    "\n",
    "        params = self.params\n",
    "        mosaic = self.mosaic and random.random() < params['mosaic']\n",
    "\n",
    "        if mosaic:\n",
    "            shapes = None\n",
    "            # Load MOSAIC\n",
    "            image, label = self.load_mosaic(index, params)\n",
    "            # MixUp augmentation\n",
    "            if random.random() < params['mix_up']:\n",
    "                index = random.choice(self.indices)\n",
    "                mix_image1, mix_label1 = image, label\n",
    "                mix_image2, mix_label2 = self.load_mosaic(index, params)\n",
    "\n",
    "                image, label = mix_up(mix_image1, mix_label1, mix_image2, mix_label2)\n",
    "        else:\n",
    "            # Load image\n",
    "            image, shape = self.load_image(index)\n",
    "            h, w = image.shape[:2]\n",
    "\n",
    "            # Resize\n",
    "            image, ratio, pad = resize(image, self.input_size, self.augment)\n",
    "            shapes = shape, ((h / shape[0], w / shape[1]), pad)  # for COCO mAP rescaling\n",
    "\n",
    "            label = self.labels[index].copy()\n",
    "            if label.size:\n",
    "                label[:, 1:] = ds_wh2xy(label[:, 1:], ratio[0] * w, ratio[1] * h, pad[0], pad[1])\n",
    "            if self.augment:\n",
    "                image, label = random_perspective(image, label, params)\n",
    "        nl = len(label)  # number of labels\n",
    "        if nl:\n",
    "            label[:, 1:5] = xy2wh(label[:, 1:5], image.shape[1], image.shape[0])\n",
    "\n",
    "        if self.augment:\n",
    "            # Albumentations\n",
    "            image, label = self.albumentations(image, label)\n",
    "            nl = len(label)  # update after albumentations\n",
    "            # HSV color-space\n",
    "            augment_hsv(image, params)\n",
    "            # Flip up-down\n",
    "            if random.random() < params['flip_ud']:\n",
    "                image = np.flipud(image)\n",
    "                if nl:\n",
    "                    label[:, 2] = 1 - label[:, 2]\n",
    "            # Flip left-right\n",
    "            if random.random() < params['flip_lr']:\n",
    "                image = np.fliplr(image)\n",
    "                if nl:\n",
    "                    label[:, 1] = 1 - label[:, 1]\n",
    "\n",
    "        target = torch.zeros((nl, 6))\n",
    "        if nl:\n",
    "            target[:, 1:] = torch.from_numpy(label)\n",
    "\n",
    "        # Convert HWC to CHW, BGR to RGB\n",
    "        sample = image.transpose((2, 0, 1))[::-1]\n",
    "        sample = np.ascontiguousarray(sample)\n",
    "\n",
    "        return torch.from_numpy(sample), target, shapes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def load_image(self, i):\n",
    "        image = cv2.imread(self.filenames[i])\n",
    "        h, w = image.shape[:2]\n",
    "        r = self.input_size / max(h, w)\n",
    "        if r != 1:\n",
    "            image = cv2.resize(image,\n",
    "                               dsize=(int(w * r), int(h * r)),\n",
    "                               interpolation=resample() if self.augment else cv2.INTER_LINEAR)\n",
    "        return image, (h, w)\n",
    "\n",
    "    def load_mosaic(self, index, params):\n",
    "        label4 = []\n",
    "        image4 = np.full((self.input_size * 2, self.input_size * 2, 3), 0, dtype=np.uint8)\n",
    "        y1a, y2a, x1a, x2a, y1b, y2b, x1b, x2b = (None, None, None, None, None, None, None, None)\n",
    "\n",
    "        border = [-self.input_size // 2, -self.input_size // 2]\n",
    "\n",
    "        xc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
    "        yc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
    "\n",
    "        indices = [index] + random.choices(self.indices, k=3)\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        for i, index in enumerate(indices):\n",
    "            # Load image\n",
    "            image, _ = self.load_image(index)\n",
    "            shape = image.shape\n",
    "            if i == 0:  # top left\n",
    "                x1a = max(xc - shape[1], 0)\n",
    "                y1a = max(yc - shape[0], 0)\n",
    "                x2a = xc\n",
    "                y2a = yc\n",
    "                x1b = shape[1] - (x2a - x1a)\n",
    "                y1b = shape[0] - (y2a - y1a)\n",
    "                x2b = shape[1]\n",
    "                y2b = shape[0]\n",
    "            if i == 1:  # top right\n",
    "                x1a = xc\n",
    "                y1a = max(yc - shape[0], 0)\n",
    "                x2a = min(xc + shape[1], self.input_size * 2)\n",
    "                y2a = yc\n",
    "                x1b = 0\n",
    "                y1b = shape[0] - (y2a - y1a)\n",
    "                x2b = min(shape[1], x2a - x1a)\n",
    "                y2b = shape[0]\n",
    "            if i == 2:  # bottom left\n",
    "                x1a = max(xc - shape[1], 0)\n",
    "                y1a = yc\n",
    "                x2a = xc\n",
    "                y2a = min(self.input_size * 2, yc + shape[0])\n",
    "                x1b = shape[1] - (x2a - x1a)\n",
    "                y1b = 0\n",
    "                x2b = shape[1]\n",
    "                y2b = min(y2a - y1a, shape[0])\n",
    "            if i == 3:  # bottom right\n",
    "                x1a = xc\n",
    "                y1a = yc\n",
    "                x2a = min(xc + shape[1], self.input_size * 2)\n",
    "                y2a = min(self.input_size * 2, yc + shape[0])\n",
    "                x1b = 0\n",
    "                y1b = 0\n",
    "                x2b = min(shape[1], x2a - x1a)\n",
    "                y2b = min(y2a - y1a, shape[0])\n",
    "\n",
    "            image4[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
    "            pad_w = x1a - x1b\n",
    "            pad_h = y1a - y1b\n",
    "\n",
    "            # Labels\n",
    "            label = self.labels[index].copy()\n",
    "            if len(label):\n",
    "                label[:, 1:] = ds_wh2xy(label[:, 1:], shape[1], shape[0], pad_w, pad_h)\n",
    "            label4.append(label)\n",
    "\n",
    "        # Concat/clip labels\n",
    "        label4 = np.concatenate(label4, 0)   \n",
    "        for x in label4[:, 1:]:\n",
    "            np.clip(x, 0, 2 * self.input_size, out=x)\n",
    "\n",
    "        # Augment\n",
    "        image4, label4 = random_perspective(image4, label4, params, border)\n",
    "\n",
    "        return image4, label4\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        samples, targets, shapes = zip(*batch)\n",
    "        for i, item in enumerate(targets):\n",
    "            item[:, 0] = i  # add target image index\n",
    "        return torch.stack(samples, 0), torch.cat(targets, 0), shapes\n",
    "\n",
    "    @staticmethod\n",
    "    def load_label(filenames):\n",
    "        path = f'{os.path.dirname(filenames[0])}.cache'\n",
    "        if os.path.exists(path):\n",
    "            return torch.load(path)\n",
    "        x = {}\n",
    "        for filename in filenames:\n",
    "            try:\n",
    "                # verify images\n",
    "                with open(filename, 'rb') as f:\n",
    "                    image = Image.open(f)\n",
    "                    image.verify()  # PIL verify\n",
    "                shape = image.size  # image size\n",
    "                assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\n",
    "                assert image.format.lower() in FORMATS, f'invalid image format {image.format}'\n",
    "\n",
    "                # verify labels\n",
    "                a = f'{os.sep}images{os.sep}'\n",
    "                b = f'{os.sep}labels{os.sep}'\n",
    "                if os.path.isfile(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt'):\n",
    "                    with open(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt') as f:\n",
    "                        label = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
    "                        label = np.array(label, dtype=np.float32)\n",
    "                    nl = len(label)\n",
    "                    if nl:\n",
    "                        assert label.shape[1] == 5, 'labels require 5 columns'\n",
    "                        assert (label >= 0).all(), 'negative label values'\n",
    "                        assert (label[:, 1:] <= 1).all(), 'non-normalized coordinates'\n",
    "                        _, i = np.unique(label, axis=0, return_index=True)\n",
    "                        if len(i) < nl:  # duplicate row check\n",
    "                            label = label[i]  # remove duplicates\n",
    "                    else:\n",
    "                        label = np.zeros((0, 5), dtype=np.float32)\n",
    "                else:\n",
    "                    label = np.zeros((0, 5), dtype=np.float32)\n",
    "                if filename:\n",
    "                    x[filename] = [label, shape]\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        torch.save(x, path)\n",
    "        return x\n",
    "\n",
    "\n",
    "def ds_wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n",
    "    # Convert nx4 boxes\n",
    "    # from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w  # top left x\n",
    "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h  # top left y\n",
    "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w  # bottom right x\n",
    "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def xy2wh(x, w=640, h=640):\n",
    "    # warning: inplace clip\n",
    "    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)  # x1, x2\n",
    "    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)  # y1, y2\n",
    "\n",
    "    # Convert nx4 boxes\n",
    "    # from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
    "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
    "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
    "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def resample():\n",
    "    choices = (cv2.INTER_AREA,\n",
    "               cv2.INTER_CUBIC,\n",
    "               cv2.INTER_LINEAR,\n",
    "               cv2.INTER_NEAREST,\n",
    "               cv2.INTER_LANCZOS4)\n",
    "    return random.choice(seq=choices)\n",
    "\n",
    "\n",
    "def augment_hsv(image, params):\n",
    "    # HSV color-space augmentation\n",
    "    h = params['hsv_h']\n",
    "    s = params['hsv_s']\n",
    "    v = params['hsv_v']\n",
    "\n",
    "    r = np.random.uniform(-1, 1, 3) * [h, s, v] + 1\n",
    "    h, s, v = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n",
    "\n",
    "    x = np.arange(0, 256, dtype=r.dtype)\n",
    "    lut_h = ((x * r[0]) % 180).astype('uint8')\n",
    "    lut_s = np.clip(x * r[1], 0, 255).astype('uint8')\n",
    "    lut_v = np.clip(x * r[2], 0, 255).astype('uint8')\n",
    "\n",
    "    im_hsv = cv2.merge((cv2.LUT(h, lut_h), cv2.LUT(s, lut_s), cv2.LUT(v, lut_v)))\n",
    "    cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=image)  # no return needed\n",
    "\n",
    "\n",
    "def resize(image, input_size, augment):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = image.shape[:2]  # current shape [height, width]\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(input_size / shape[0], input_size / shape[1])\n",
    "    if not augment:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    pad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    w = (input_size - pad[0]) / 2\n",
    "    h = (input_size - pad[1]) / 2\n",
    "\n",
    "    if shape[::-1] != pad:  # resize\n",
    "        image = cv2.resize(image,\n",
    "                           dsize=pad,\n",
    "                           interpolation=resample() if augment else cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(h - 0.1)), int(round(h + 0.1))\n",
    "    left, right = int(round(w - 0.1)), int(round(w + 0.1))\n",
    "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)  # add border\n",
    "    return image, (r, r), (w, h)\n",
    "\n",
    "\n",
    "def candidates(box1, box2):\n",
    "    # box1(4,n), box2(4,n)\n",
    "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
    "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
    "    aspect_ratio = np.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))  # aspect ratio\n",
    "    return (w2 > 2) & (h2 > 2) & (w2 * h2 / (w1 * h1 + 1e-16) > 0.1) & (aspect_ratio < 100)\n",
    "\n",
    "\n",
    "def random_perspective(samples, targets, params, border=(0, 0)):\n",
    "    h = samples.shape[0] + border[0] * 2\n",
    "    w = samples.shape[1] + border[1] * 2\n",
    "\n",
    "    # Center\n",
    "    center = np.eye(3)\n",
    "    center[0, 2] = -samples.shape[1] / 2  # x translation (pixels)\n",
    "    center[1, 2] = -samples.shape[0] / 2  # y translation (pixels)\n",
    "\n",
    "    # Perspective\n",
    "    perspective = np.eye(3)\n",
    "\n",
    "    # Rotation and Scale\n",
    "    rotate = np.eye(3)\n",
    "    a = random.uniform(-params['degrees'], params['degrees'])\n",
    "    s = random.uniform(1 - params['scale'], 1 + params['scale'])\n",
    "    rotate[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
    "\n",
    "    # Shear\n",
    "    shear = np.eye(3)\n",
    "    shear[0, 1] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
    "    shear[1, 0] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
    "\n",
    "    # Translation\n",
    "    translate = np.eye(3)\n",
    "    translate[0, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * w\n",
    "    translate[1, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * h\n",
    "\n",
    "    # Combined rotation matrix, order of operations (right to left) is IMPORTANT\n",
    "    matrix = translate @ shear @ rotate @ perspective @ center\n",
    "    if (border[0] != 0) or (border[1] != 0) or (matrix != np.eye(3)).any():  # image changed\n",
    "        samples = cv2.warpAffine(samples, matrix[:2], dsize=(w, h), borderValue=(0, 0, 0))\n",
    "\n",
    "    # Transform label coordinates\n",
    "    n = len(targets)\n",
    "    if n:\n",
    "        xy = np.ones((n * 4, 3))\n",
    "        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
    "        xy = xy @ matrix.T  # transform\n",
    "        xy = xy[:, :2].reshape(n, 8)  # perspective rescale or affine\n",
    "\n",
    "        # create new boxes\n",
    "        x = xy[:, [0, 2, 4, 6]]\n",
    "        y = xy[:, [1, 3, 5, 7]]\n",
    "        new = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
    "\n",
    "        # clip\n",
    "        new[:, [0, 2]] = new[:, [0, 2]].clip(0, w)\n",
    "        new[:, [1, 3]] = new[:, [1, 3]].clip(0, h)\n",
    "\n",
    "        # filter candidates\n",
    "        indices = candidates(box1=targets[:, 1:5].T * s, box2=new.T)\n",
    "        targets = targets[indices]\n",
    "        targets[:, 1:5] = new[indices]\n",
    "\n",
    "    return samples, targets\n",
    "\n",
    "\n",
    "def mix_up(image1, label1, image2, label2):\n",
    "    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n",
    "    alpha = np.random.beta(32.0, 32.0)  # mix-up ratio, alpha=beta=32.0\n",
    "    image = (image1 * alpha + image2 * (1 - alpha)).astype(np.uint8)\n",
    "    label = np.concatenate((label1, label2), 0)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "class Albumentations:\n",
    "    def __init__(self):\n",
    "        self.transform = None\n",
    "        try:\n",
    "            import albumentations as album\n",
    "\n",
    "            transforms = [album.Blur(p=0.01),\n",
    "                          album.CLAHE(p=0.01),\n",
    "                          album.ToGray(p=0.01),\n",
    "                          album.MedianBlur(p=0.01)]\n",
    "            self.transform = album.Compose(transforms,\n",
    "                                           album.BboxParams('yolo', ['class_labels']))\n",
    "\n",
    "        except ImportError:  # package not installed, skip\n",
    "            pass\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        if self.transform:\n",
    "            x = self.transform(image=image,\n",
    "                               bboxes=label[:, 1:],\n",
    "                               class_labels=label[:, 0])\n",
    "            image = x['image']\n",
    "            label = np.array([[c, *b] for c, b in zip(x['class_labels'], x['bboxes'])])\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5254a",
   "metadata": {},
   "source": [
    "\n",
    "## Utility Functions\n",
    "### Seed and Multi-Processing Setup\n",
    "- `setup_seed`: Ensures reproducibility by setting seeds for random operations.\n",
    "- `setup_multi_processes`: Configures environment variables for multi-processing, optimizing performance during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bfbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed():\n",
    "    \"\"\"\n",
    "    Setup random seed.\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def setup_multi_processes():\n",
    "    \"\"\"\n",
    "    Setup multi-processing environment variables.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    from os import environ\n",
    "    from platform import system\n",
    "\n",
    "    # set multiprocess start method as `fork` to speed up the training\n",
    "    if system() != 'Windows':\n",
    "        torch.multiprocessing.set_start_method('fork', force=True)\n",
    "\n",
    "    # disable opencv multithreading to avoid system being overloaded\n",
    "    cv2.setNumThreads(0)\n",
    "\n",
    "    # setup OMP threads\n",
    "    if 'OMP_NUM_THREADS' not in environ:\n",
    "        environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    # setup MKL threads\n",
    "    if 'MKL_NUM_THREADS' not in environ:\n",
    "        environ['MKL_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b4cf5",
   "metadata": {},
   "source": [
    "## Loss, Metrics, & Optimization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ac5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(coords, shape1, shape2, ratio_pad=None):\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(shape1[0] / shape2[0], shape1[1] / shape2[1])  # gain  = old / new\n",
    "        pad = (shape1[1] - shape2[1] * gain) / 2, (shape1[0] - shape2[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
    "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
    "    coords[:, :4] /= gain\n",
    "\n",
    "    coords[:, 0].clamp_(0, shape2[1])  # x1\n",
    "    coords[:, 1].clamp_(0, shape2[0])  # y1\n",
    "    coords[:, 2].clamp_(0, shape2[1])  # x2\n",
    "    coords[:, 3].clamp_(0, shape2[0])  # y2\n",
    "    return coords\n",
    "\n",
    "\n",
    "def make_anchors(x, strides, offset=0.5):\n",
    "    \"\"\"\n",
    "    Generate anchors from features\n",
    "    \"\"\"\n",
    "    assert x is not None\n",
    "    anchor_points, stride_tensor = [], []\n",
    "    for i, stride in enumerate(strides):\n",
    "        _, _, h, w = x[i].shape\n",
    "        sx = torch.arange(end=w, dtype=x[i].dtype, device=x[i].device) + offset  # shift x\n",
    "        sy = torch.arange(end=h, dtype=x[i].dtype, device=x[i].device) + offset  # shift y\n",
    "        sy, sx = torch.meshgrid(sy, sx)\n",
    "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=x[i].dtype, device=x[i].device))\n",
    "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
    "\n",
    "\n",
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    # intersection(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    (a1, a2), (b1, b2) = box1[:, None].chunk(2, 2), box2.chunk(2, 1)\n",
    "    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "\n",
    "    # IoU = intersection / (area1 + area2 - intersection)\n",
    "    box1 = box1.T\n",
    "    box2 = box2.T\n",
    "\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    return intersection / (area1[:, None] + area2 - intersection)\n",
    "\n",
    "\n",
    "def wh2xy(x):\n",
    "    y = x.clone()\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, conf_threshold=0.25, iou_threshold=0.45):\n",
    "    nc = prediction.shape[1] - 4  # number of classes\n",
    "    xc = prediction[:, 4:4 + nc].amax(1) > conf_threshold  # candidates\n",
    "\n",
    "    # Settings\n",
    "    max_wh = 7680  # (pixels) maximum box width and height\n",
    "    max_det = 300  # the maximum number of boxes to keep after NMS\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "\n",
    "    start = time.time()\n",
    "    outputs = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
    "    for index, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        x = x.transpose(0, -1)[xc[index]]  # confidence\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Detections matrix nx6 (box, conf, cls)\n",
    "        box, cls = x.split((4, nc), 1)\n",
    "        # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "        box = wh2xy(box)\n",
    "        if nc > 1:\n",
    "            i, j = (cls > conf_threshold).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = cls.max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_threshold]\n",
    "        # Check shape\n",
    "        if not x.shape[0]:  # no boxes\n",
    "            continue\n",
    "        # sort by confidence and remove excess boxes\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * max_wh  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_threshold)  # NMS\n",
    "        i = i[:max_det]  # limit detections\n",
    "        outputs[index] = x[i]\n",
    "        if (time.time() - start) > 0.5 + 0.05 * prediction.shape[0]:\n",
    "            print(f'WARNING ⚠️ NMS time limit {0.5 + 0.05 * prediction.shape[0]:.3f}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def smooth(y, f=0.05):\n",
    "    # Box filter of fraction f\n",
    "    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n",
    "    p = np.ones(nf // 2)  # ones padding\n",
    "    yp = np.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n",
    "    return np.convolve(yp, np.ones(nf) / nf, mode='valid')  # y-smoothed\n",
    "\n",
    "\n",
    "def compute_ap(tp, conf, pred_cls, target_cls, eps=1e-16):\n",
    "    \"\"\"\n",
    "    Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:  True positives (nparray, nx1 or nx10).\n",
    "        conf:  Object-ness value from 0-1 (nparray).\n",
    "        pred_cls:  Predicted object classes (nparray).\n",
    "        target_cls:  True object classes (nparray).\n",
    "    # Returns\n",
    "        The average precision\n",
    "    \"\"\"\n",
    "    # Sort by object-ness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = np.unique(target_cls, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    p = np.zeros((nc, 1000))\n",
    "    r = np.zeros((nc, 1000))\n",
    "    ap = np.zeros((nc, tp.shape[1]))\n",
    "    px, py = np.linspace(0, 1, 1000), []  # for plotting\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        nl = nt[ci]  # number of labels\n",
    "        no = i.sum()  # number of outputs\n",
    "        if no == 0 or nl == 0:\n",
    "            continue\n",
    "\n",
    "        # Accumulate FPs and TPs\n",
    "        fpc = (1 - tp[i]).cumsum(0)\n",
    "        tpc = tp[i].cumsum(0)\n",
    "\n",
    "        # Recall\n",
    "        recall = tpc / (nl + eps)  # recall curve\n",
    "        # negative x, xp because xp decreases\n",
    "        r[ci] = np.interp(-px, -conf[i], recall[:, 0], left=0)\n",
    "\n",
    "        # Precision\n",
    "        precision = tpc / (tpc + fpc)  # precision curve\n",
    "        p[ci] = np.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "        # AP from recall-precision curve\n",
    "        for j in range(tp.shape[1]):\n",
    "            m_rec = np.concatenate(([0.0], recall[:, j], [1.0]))\n",
    "            m_pre = np.concatenate(([1.0], precision[:, j], [0.0]))\n",
    "\n",
    "            # Compute the precision envelope\n",
    "            m_pre = np.flip(np.maximum.accumulate(np.flip(m_pre)))\n",
    "\n",
    "            # Integrate area under curve\n",
    "            x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
    "            ap[ci, j] = np.trapz(np.interp(x, m_rec, m_pre), x)  # integrate\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1 = 2 * p * r / (p + r + eps)\n",
    "\n",
    "    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n",
    "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "    m_pre, m_rec = p.mean(), r.mean()\n",
    "    map50, mean_ap = ap50.mean(), ap.mean()\n",
    "    return tp, fp, m_pre, m_rec, map50, mean_ap\n",
    "\n",
    "\n",
    "def strip_optimizer(filename):\n",
    "    x = torch.load(filename, map_location=torch.device('cpu'))\n",
    "    x['model'].half()  # to FP16\n",
    "    for p in x['model'].parameters():\n",
    "        p.requires_grad = False\n",
    "    torch.save(x, filename)\n",
    "\n",
    "\n",
    "def clip_gradients(model, max_norm=10.0):\n",
    "    parameters = model.parameters()\n",
    "    torch.nn.utils.clip_grad_norm_(parameters, max_norm=max_norm)\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"\n",
    "    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n",
    "    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n",
    "    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n",
    "        # Create EMA\n",
    "        self.ema = copy.deepcopy(model).eval()  # FP32 EMA\n",
    "        self.updates = updates  # number of EMA updates\n",
    "        # decay exponential ramp (to help early epochs)\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "        # Update EMA parameters\n",
    "        with torch.no_grad():\n",
    "            self.updates += 1\n",
    "            d = self.decay(self.updates)\n",
    "\n",
    "            msd = model.state_dict()  # model state_dict\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1 - d) * msd[k].detach()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        self.sum = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, v, n):\n",
    "        if not math.isnan(float(v)):\n",
    "            self.num = self.num + n\n",
    "            self.sum = self.sum + v * n\n",
    "            self.avg = self.sum / self.num\n",
    "\n",
    "\n",
    "class ComputeLoss:\n",
    "    def __init__(self, model, params):\n",
    "        super().__init__()\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "\n",
    "        m = model.head  # Head() module\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.stride = m.stride  # model strides\n",
    "        self.nc = m.nc  # number of classes\n",
    "        self.no = m.no\n",
    "        self.device = device\n",
    "        self.params = params\n",
    "\n",
    "        # task aligned assigner\n",
    "        self.top_k = 10\n",
    "        self.alpha = 0.5\n",
    "        self.beta = 6.0\n",
    "        self.eps = 1e-9\n",
    "\n",
    "        self.bs = 1\n",
    "        self.num_max_boxes = 0\n",
    "        # DFL Loss params\n",
    "        self.dfl_ch = m.dfl.ch\n",
    "        self.project = torch.arange(self.dfl_ch, dtype=torch.float, device=device)\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        x = outputs[1] if isinstance(outputs, tuple) else outputs\n",
    "        output = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n",
    "        pred_output, pred_scores = output.split((4 * self.dfl_ch, self.nc), 1)\n",
    "\n",
    "        pred_output = pred_output.permute(0, 2, 1).contiguous()\n",
    "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        size = torch.tensor(x[0].shape[2:], dtype=pred_scores.dtype, device=self.device)\n",
    "        size = size * self.stride[0]\n",
    "\n",
    "        anchor_points, stride_tensor = make_anchors(x, self.stride, 0.5)\n",
    "\n",
    "        # targets\n",
    "        if targets.shape[0] == 0:\n",
    "            gt = torch.zeros(pred_scores.shape[0], 0, 5, device=self.device)\n",
    "        else:\n",
    "            i = targets[:, 0]  # image index\n",
    "            _, counts = i.unique(return_counts=True)\n",
    "            gt = torch.zeros(pred_scores.shape[0], counts.max(), 5, device=self.device)\n",
    "            for j in range(pred_scores.shape[0]):\n",
    "                matches = i == j\n",
    "                n = matches.sum()\n",
    "                if n:\n",
    "                    gt[j, :n] = targets[matches, 1:]\n",
    "            gt[..., 1:5] = wh2xy(gt[..., 1:5].mul_(size[[1, 0, 1, 0]]))\n",
    "\n",
    "        gt_labels, gt_bboxes = gt.split((1, 4), 2)  # cls, xyxy\n",
    "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n",
    "\n",
    "        # boxes\n",
    "        b, a, c = pred_output.shape\n",
    "        pred_bboxes = pred_output.view(b, a, 4, c // 4).softmax(3)\n",
    "        pred_bboxes = pred_bboxes.matmul(self.project.type(pred_bboxes.dtype))\n",
    "\n",
    "        a, b = torch.split(pred_bboxes, 2, -1)\n",
    "        pred_bboxes = torch.cat((anchor_points - a, anchor_points + b), -1)\n",
    "\n",
    "        scores = pred_scores.detach().sigmoid()\n",
    "        bboxes = (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype)\n",
    "        target_bboxes, target_scores, fg_mask = self.assign(scores, bboxes,\n",
    "                                                            gt_labels, gt_bboxes, mask_gt,\n",
    "                                                            anchor_points * stride_tensor)\n",
    "\n",
    "        target_bboxes /= stride_tensor\n",
    "        target_scores_sum = target_scores.sum()\n",
    "\n",
    "        # cls loss\n",
    "        loss_cls = self.bce(pred_scores, target_scores.to(pred_scores.dtype))\n",
    "        loss_cls = loss_cls.sum() / target_scores_sum\n",
    "\n",
    "        # box loss\n",
    "        loss_box = torch.zeros(1, device=self.device)\n",
    "        loss_dfl = torch.zeros(1, device=self.device)\n",
    "        if fg_mask.sum():\n",
    "            # IoU loss\n",
    "            weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
    "            loss_box = self.iou(pred_bboxes[fg_mask], target_bboxes[fg_mask])\n",
    "            loss_box = ((1.0 - loss_box) * weight).sum() / target_scores_sum\n",
    "            # DFL loss\n",
    "            a, b = torch.split(target_bboxes, 2, -1)\n",
    "            target_lt_rb = torch.cat((anchor_points - a, b - anchor_points), -1)\n",
    "            target_lt_rb = target_lt_rb.clamp(0, self.dfl_ch - 1.01)  # distance (left_top, right_bottom)\n",
    "            loss_dfl = self.df_loss(pred_output[fg_mask].view(-1, self.dfl_ch), target_lt_rb[fg_mask])\n",
    "            loss_dfl = (loss_dfl * weight).sum() / target_scores_sum\n",
    "\n",
    "        loss_cls *= self.params['cls']\n",
    "        loss_box *= self.params['box']\n",
    "        loss_dfl *= self.params['dfl']\n",
    "        return loss_cls + loss_box + loss_dfl  # loss(cls, box, dfl)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def assign(self, pred_scores, pred_bboxes, true_labels, true_bboxes, true_mask, anchors):\n",
    "        \"\"\"\n",
    "        Task-aligned One-stage Object Detection assigner\n",
    "        \"\"\"\n",
    "        self.bs = pred_scores.size(0)\n",
    "        self.num_max_boxes = true_bboxes.size(1)\n",
    "\n",
    "        if self.num_max_boxes == 0:\n",
    "            device = true_bboxes.device\n",
    "            return (torch.full_like(pred_scores[..., 0], self.nc).to(device),\n",
    "                    torch.zeros_like(pred_bboxes).to(device),\n",
    "                    torch.zeros_like(pred_scores).to(device),\n",
    "                    torch.zeros_like(pred_scores[..., 0]).to(device),\n",
    "                    torch.zeros_like(pred_scores[..., 0]).to(device))\n",
    "\n",
    "        i = torch.zeros([2, self.bs, self.num_max_boxes], dtype=torch.long)\n",
    "        i[0] = torch.arange(end=self.bs).view(-1, 1).repeat(1, self.num_max_boxes)\n",
    "        i[1] = true_labels.long().squeeze(-1)\n",
    "\n",
    "        overlaps = self.iou(true_bboxes.unsqueeze(2), pred_bboxes.unsqueeze(1))\n",
    "        overlaps = overlaps.squeeze(3).clamp(0)\n",
    "        align_metric = pred_scores[i[0], :, i[1]].pow(self.alpha) * overlaps.pow(self.beta)\n",
    "        bs, n_boxes, _ = true_bboxes.shape\n",
    "        lt, rb = true_bboxes.view(-1, 1, 4).chunk(2, 2)  # left-top, right-bottom\n",
    "        bbox_deltas = torch.cat((anchors[None] - lt, rb - anchors[None]), dim=2)\n",
    "        mask_in_gts = bbox_deltas.view(bs, n_boxes, anchors.shape[0], -1).amin(3).gt_(1e-9)\n",
    "        metrics = align_metric * mask_in_gts\n",
    "        top_k_mask = true_mask.repeat([1, 1, self.top_k]).bool()\n",
    "        num_anchors = metrics.shape[-1]\n",
    "        top_k_metrics, top_k_indices = torch.topk(metrics, self.top_k, dim=-1, largest=True)\n",
    "        if top_k_mask is None:\n",
    "            top_k_mask = (top_k_metrics.max(-1, keepdim=True) > self.eps).tile([1, 1, self.top_k])\n",
    "        top_k_indices = torch.where(top_k_mask, top_k_indices, 0)\n",
    "        is_in_top_k = one_hot(top_k_indices, num_anchors).sum(-2)\n",
    "        # filter invalid boxes\n",
    "        is_in_top_k = torch.where(is_in_top_k > 1, 0, is_in_top_k)\n",
    "        mask_top_k = is_in_top_k.to(metrics.dtype)\n",
    "        # merge all mask to a final mask, (b, max_num_obj, h*w)\n",
    "        mask_pos = mask_top_k * mask_in_gts * true_mask\n",
    "\n",
    "        fg_mask = mask_pos.sum(-2)\n",
    "        if fg_mask.max() > 1:  # one anchor is assigned to multiple gt_bboxes\n",
    "            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).repeat([1, self.num_max_boxes, 1])\n",
    "            max_overlaps_idx = overlaps.argmax(1)\n",
    "            is_max_overlaps = one_hot(max_overlaps_idx, self.num_max_boxes)\n",
    "            is_max_overlaps = is_max_overlaps.permute(0, 2, 1).to(overlaps.dtype)\n",
    "            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos)\n",
    "            fg_mask = mask_pos.sum(-2)\n",
    "        # find each grid serve which gt(index)\n",
    "        target_gt_idx = mask_pos.argmax(-2)  # (b, h*w)\n",
    "\n",
    "        # assigned target labels, (b, 1)\n",
    "        batch_index = torch.arange(end=self.bs,\n",
    "                                   dtype=torch.int64,\n",
    "                                   device=true_labels.device)[..., None]\n",
    "        target_gt_idx = target_gt_idx + batch_index * self.num_max_boxes\n",
    "        target_labels = true_labels.long().flatten()[target_gt_idx]\n",
    "\n",
    "        # assigned target boxes\n",
    "        target_bboxes = true_bboxes.view(-1, 4)[target_gt_idx]\n",
    "\n",
    "        # assigned target scores\n",
    "        target_labels.clamp(0)\n",
    "        target_scores = one_hot(target_labels, self.nc)\n",
    "        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.nc)\n",
    "        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
    "\n",
    "        # normalize\n",
    "        align_metric *= mask_pos\n",
    "        pos_align_metrics = align_metric.amax(axis=-1, keepdim=True)\n",
    "        pos_overlaps = (overlaps * mask_pos).amax(axis=-1, keepdim=True)\n",
    "        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2)\n",
    "        norm_align_metric = norm_align_metric.unsqueeze(-1)\n",
    "        target_scores = target_scores * norm_align_metric\n",
    "\n",
    "        return target_bboxes, target_scores, fg_mask.bool()\n",
    "\n",
    "    @staticmethod\n",
    "    def df_loss(pred_dist, target):\n",
    "        # Return sum of left and right DFL losses\n",
    "        # Distribution Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "        tl = target.long()  # target left\n",
    "        tr = tl + 1  # target right\n",
    "        wl = tr - target  # weight left\n",
    "        wr = 1 - wl  # weight right\n",
    "        l_loss = cross_entropy(pred_dist, tl.view(-1), reduction=\"none\").view(tl.shape)\n",
    "        r_loss = cross_entropy(pred_dist, tr.view(-1), reduction=\"none\").view(tl.shape)\n",
    "        return (l_loss * wl + r_loss * wr).mean(-1, keepdim=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def iou(box1, box2, eps=1e-7):\n",
    "        # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n",
    "\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
    "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "\n",
    "        # Intersection area\n",
    "        area1 = b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)\n",
    "        area2 = b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)\n",
    "        intersection = area1.clamp(0) * area2.clamp(0)\n",
    "\n",
    "        # Union Area\n",
    "        union = w1 * h1 + w2 * h2 - intersection + eps\n",
    "\n",
    "        # IoU\n",
    "        iou = intersection / union\n",
    "        cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex width\n",
    "        ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n",
    "        # Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "        c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "        # center dist ** 2\n",
    "        rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n",
    "        # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "        v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n",
    "        with torch.no_grad():\n",
    "            alpha = v / (v - iou + (1 + eps))\n",
    "        return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d915ffa",
   "metadata": {},
   "source": [
    "\n",
    "## Model Components\n",
    "### Convolutional Block (`Conv`)\n",
    "- This block includes a convolutional layer, batch normalization, and a SiLU activation function.\n",
    "- The `fuse_forward` method allows for a more efficient forward pass by fusing the convolution with batch normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(k, p=None, d=1):\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1\n",
    "    if p is None:\n",
    "        p = k // 2\n",
    "    return p\n",
    "\n",
    "\n",
    "def fuse_conv(conv, norm):\n",
    "    fused_conv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                 conv.out_channels,\n",
    "                                 kernel_size=conv.kernel_size,\n",
    "                                 stride=conv.stride,\n",
    "                                 padding=conv.padding,\n",
    "                                 groups=conv.groups,\n",
    "                                 bias=True).requires_grad_(False).to(conv.weight.device)\n",
    "\n",
    "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "    w_norm = torch.diag(norm.weight.div(torch.sqrt(norm.eps + norm.running_var)))\n",
    "    fused_conv.weight.copy_(torch.mm(w_norm, w_conv).view(fused_conv.weight.size()))\n",
    "\n",
    "    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n",
    "    b_norm = norm.bias - norm.weight.mul(norm.running_mean).div(torch.sqrt(norm.running_var + norm.eps))\n",
    "    fused_conv.bias.copy_(torch.mm(w_norm, b_conv.reshape(-1, 1)).reshape(-1) + b_norm)\n",
    "\n",
    "    return fused_conv\n",
    "\n",
    "\n",
    "class Conv(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=1, s=1, p=None, d=1, g=1):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch, out_ch, k, s, pad(k, p, d), d, g, False)\n",
    "        self.norm = torch.nn.BatchNorm2d(out_ch, 0.001, 0.03)\n",
    "        self.relu = torch.nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.norm(self.conv(x)))\n",
    "\n",
    "    def fuse_forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, ch, add=True):\n",
    "        super().__init__()\n",
    "        self.add_m = add\n",
    "        self.res_m = torch.nn.Sequential(Conv(ch, ch, 3),\n",
    "                                         Conv(ch, ch, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.res_m(x) + x if self.add_m else self.res_m(x)\n",
    "\n",
    "\n",
    "class CSP(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, n=1, add=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_ch, out_ch // 2)\n",
    "        self.conv2 = Conv(in_ch, out_ch // 2)\n",
    "        self.conv3 = Conv((2 + n) * out_ch // 2, out_ch)\n",
    "        self.res_m = torch.nn.ModuleList(Residual(out_ch // 2, add) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = [self.conv1(x), self.conv2(x)]\n",
    "        y.extend(m(y[-1]) for m in self.res_m)\n",
    "        return self.conv3(torch.cat(y, dim=1))\n",
    "\n",
    "\n",
    "class SPP(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_ch, in_ch // 2)\n",
    "        self.conv2 = Conv(in_ch * 2, out_ch)\n",
    "        self.res_m = torch.nn.MaxPool2d(k, 1, k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y1 = self.res_m(x)\n",
    "        y2 = self.res_m(y1)\n",
    "        return self.conv2(torch.cat([x, y1, y2, self.res_m(y2)], 1))\n",
    "\n",
    "\n",
    "class DarkNet(torch.nn.Module):\n",
    "    def __init__(self, width, depth):\n",
    "        super().__init__()\n",
    "        p1 = [Conv(width[0], width[1], 3, 2)]\n",
    "        p2 = [Conv(width[1], width[2], 3, 2),\n",
    "              CSP(width[2], width[2], depth[0])]\n",
    "        p3 = [Conv(width[2], width[3], 3, 2),\n",
    "              CSP(width[3], width[3], depth[1])]\n",
    "        p4 = [Conv(width[3], width[4], 3, 2),\n",
    "              CSP(width[4], width[4], depth[2])]\n",
    "        p5 = [Conv(width[4], width[5], 3, 2),\n",
    "              CSP(width[5], width[5], depth[0]),\n",
    "              SPP(width[5], width[5])]\n",
    "\n",
    "        self.p1 = torch.nn.Sequential(*p1)\n",
    "        self.p2 = torch.nn.Sequential(*p2)\n",
    "        self.p3 = torch.nn.Sequential(*p3)\n",
    "        self.p4 = torch.nn.Sequential(*p4)\n",
    "        self.p5 = torch.nn.Sequential(*p5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1(x)\n",
    "        p2 = self.p2(p1)\n",
    "        p3 = self.p3(p2)\n",
    "        p4 = self.p4(p3)\n",
    "        p5 = self.p5(p4)\n",
    "        return p3, p4, p5\n",
    "\n",
    "\n",
    "class DarkFPN(torch.nn.Module):\n",
    "    def __init__(self, width, depth):\n",
    "        super().__init__()\n",
    "        self.up = torch.nn.Upsample(None, 2)\n",
    "        self.h1 = CSP(width[4] + width[5], width[4], depth[0], False)\n",
    "        self.h2 = CSP(width[3] + width[4], width[3], depth[0], False)\n",
    "        self.h3 = Conv(width[3], width[3], 3, 2)\n",
    "        self.h4 = CSP(width[3] + width[4], width[4], depth[0], False)\n",
    "        self.h5 = Conv(width[4], width[4], 3, 2)\n",
    "        self.h6 = CSP(width[4] + width[5], width[5], depth[0], False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p3, p4, p5 = x\n",
    "        h1 = self.h1(torch.cat([self.up(p5), p4], 1))\n",
    "        h2 = self.h2(torch.cat([self.up(h1), p3], 1))\n",
    "        h4 = self.h4(torch.cat([self.h3(h2), h1], 1))\n",
    "        h6 = self.h6(torch.cat([self.h5(h4), p5], 1))\n",
    "        return h2, h4, h6\n",
    "\n",
    "\n",
    "class DFL(torch.nn.Module):\n",
    "    # Integral module of Distribution Focal Loss (DFL)\n",
    "    # Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "    def __init__(self, ch=16):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.conv = torch.nn.Conv2d(ch, 1, 1, bias=False).requires_grad_(False)\n",
    "        x = torch.arange(ch, dtype=torch.float).view(1, ch, 1, 1)\n",
    "        self.conv.weight.data[:] = torch.nn.Parameter(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, a = x.shape\n",
    "        x = x.view(b, 4, self.ch, a).transpose(2, 1)\n",
    "        return self.conv(x.softmax(1)).view(b, 4, a)\n",
    "\n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "    anchors = torch.empty(0)\n",
    "    strides = torch.empty(0)\n",
    "\n",
    "    def __init__(self, nc=80, filters=()):\n",
    "        super().__init__()\n",
    "        self.ch = 16  # DFL channels\n",
    "        self.nc = nc  # number of classes\n",
    "        self.nl = len(filters)  # number of detection layers\n",
    "        self.no = nc + self.ch * 4  # number of outputs per anchor\n",
    "        self.stride = torch.zeros(self.nl)  # strides computed during build\n",
    "\n",
    "        c1 = max(filters[0], self.nc)\n",
    "        c2 = max((filters[0] // 4, self.ch * 4))\n",
    "\n",
    "        self.dfl = DFL(self.ch)\n",
    "        self.cls = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c1, 3),\n",
    "                                                           Conv(c1, c1, 3),\n",
    "                                                           torch.nn.Conv2d(c1, self.nc, 1)) for x in filters)\n",
    "        self.box = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c2, 3),\n",
    "                                                           Conv(c2, c2, 3),\n",
    "                                                           torch.nn.Conv2d(c2, 4 * self.ch, 1)) for x in filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.nl):\n",
    "            x[i] = torch.cat((self.box[i](x[i]), self.cls[i](x[i])), 1)\n",
    "        if self.training:\n",
    "            return x\n",
    "        self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
    "\n",
    "        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n",
    "        box, cls = x.split((self.ch * 4, self.nc), 1)\n",
    "        a, b = torch.split(self.dfl(box), 2, 1)\n",
    "        a = self.anchors.unsqueeze(0) - a\n",
    "        b = self.anchors.unsqueeze(0) + b\n",
    "        box = torch.cat(((a + b) / 2, b - a), 1)\n",
    "        return torch.cat((box * self.strides, cls.sigmoid()), 1)\n",
    "\n",
    "    def initialize_biases(self):\n",
    "        # Initialize biases\n",
    "        # WARNING: requires stride availability\n",
    "        m = self\n",
    "        for a, b, s in zip(m.box, m.cls, m.stride):\n",
    "            a[-1].bias.data[:] = 1.0  # box\n",
    "            # cls (.01 objects, 80 classes, 640 img)\n",
    "            b[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2145f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO(torch.nn.Module):\n",
    "    def __init__(self, width, depth, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = DarkNet(width, depth)\n",
    "        self.fpn = DarkFPN(width, depth)\n",
    "\n",
    "        img_dummy = torch.zeros(1, 3, 256, 256)\n",
    "        self.head = Head(num_classes, (width[3], width[4], width[5]))\n",
    "        self.head.stride = torch.tensor([256 / x.shape[-2] for x in self.forward(img_dummy)])\n",
    "        self.stride = self.head.stride\n",
    "        self.head.initialize_biases()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.fpn(x)\n",
    "        return self.head(list(x))\n",
    "\n",
    "    def fuse(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) is Conv and hasattr(m, 'norm'):\n",
    "                m.conv = fuse_conv(m.conv, m.norm)\n",
    "                m.forward = m.fuse_forward\n",
    "                delattr(m, 'norm')\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc97b9",
   "metadata": {},
   "source": [
    "\n",
    "## YOLO Model Variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85de71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_v8_n(num_classes: int = 80):\n",
    "    depth = [1, 2, 2]\n",
    "    width = [3, 16, 32, 64, 128, 256]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_s(num_classes: int = 80):\n",
    "    depth = [1, 2, 2]\n",
    "    width = [3, 32, 64, 128, 256, 512]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_m(num_classes: int = 80):\n",
    "    depth = [2, 4, 4]\n",
    "    width = [3, 48, 96, 192, 384, 576]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_l(num_classes: int = 80):\n",
    "    depth = [3, 6, 6]\n",
    "    width = [3, 64, 128, 256, 512, 512]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_x(num_classes: int = 80):\n",
    "    depth = [3, 6, 6]\n",
    "    width = [3, 80, 160, 320, 640, 640]\n",
    "    return YOLO(width, depth, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df58f5e",
   "metadata": {},
   "source": [
    "\n",
    "## Training and Testing Functions\n",
    "### `train`\n",
    "- Handles the training loop for the YOLO model, including data loading, model updates, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(epochs, params):\n",
    "    def fn(x):\n",
    "        return (1 - x / epochs) * (1.0 - params['lrf']) + params['lrf']\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "def train(dataset_path, device, input_size, epochs, batch_size, num_workers, local_rank, params):\n",
    "    if device == 'cuda' or 'gpu':\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    # Model\n",
    "    model = yolo_v8_n(len(params['names'].values()))\n",
    "    if device == 'cuda':\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    accumulate = max(round(64 / batch_size), 1)\n",
    "    params['weight_decay'] *= batch_size * accumulate / 64\n",
    "\n",
    "    p = [], [], []\n",
    "    for v in model.modules():\n",
    "        if hasattr(v, 'bias') and isinstance(v.bias, torch.nn.Parameter):\n",
    "            p[2].append(v.bias)\n",
    "        if isinstance(v, torch.nn.BatchNorm2d):\n",
    "            p[1].append(v.weight)\n",
    "        elif hasattr(v, 'weight') and isinstance(v.weight, torch.nn.Parameter):\n",
    "            p[0].append(v.weight)\n",
    "\n",
    "    optimizer = torch.optim.SGD(p[2], params['lr0'], params['momentum'], nesterov=True)\n",
    "\n",
    "    optimizer.add_param_group({'params': p[0], 'weight_decay': params['weight_decay']})\n",
    "    optimizer.add_param_group({'params': p[1]})\n",
    "    del p\n",
    "\n",
    "    # Scheduler\n",
    "    lr = learning_rate(epochs, params)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr, last_epoch=-1)\n",
    "\n",
    "    # EMA\n",
    "    ema = EMA(model) if local_rank == 0 else None\n",
    "\n",
    "    filenames = []\n",
    "    try:\n",
    "        with open(f'{dataset_path}/train2017.txt') as reader:\n",
    "            tmp_filenames = reader.readlines()\n",
    "    except FileNotFoundError:\n",
    "        tmp_filenames = os.listdir(f\"{dataset_path}/images/train2017\")\n",
    "    for filename in tmp_filenames:\n",
    "        filename = filename.rstrip().split('/')[-1]\n",
    "        filenames.append(f'{dataset_path}/images/train2017/' + filename)\n",
    "\n",
    "    dataset = Dataset(filenames, input_size, params, True)\n",
    "\n",
    "    sampler = None\n",
    "\n",
    "    loader = data.DataLoader(dataset, batch_size, sampler is None, sampler,\n",
    "                             num_workers=num_workers, pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "\n",
    "\n",
    "    # Start training\n",
    "    best = 0\n",
    "    num_batch = len(loader)\n",
    "    amp_scale = torch.GradScaler(device)\n",
    "    criterion = ComputeLoss(model, params)\n",
    "    num_warmup = max(round(params['warmup_epochs'] * num_batch), 1000)\n",
    "    with open('weights/step.csv', 'w') as f:\n",
    "        if local_rank == 0:\n",
    "            writer = csv.DictWriter(f, fieldnames=['epoch', 'mAP@50', 'mAP'])\n",
    "            writer.writeheader()\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "\n",
    "            if epochs - epoch == 10:\n",
    "                loader.dataset.mosaic = False\n",
    "\n",
    "            m_loss = AverageMeter()\n",
    "            p_bar = enumerate(loader)\n",
    "            if local_rank == 0:\n",
    "                print(('\\n' + '%10s' * 3) % ('epoch', 'memory', 'loss'))\n",
    "            if local_rank == 0:\n",
    "                p_bar = tqdm.tqdm(p_bar, total=num_batch)  # progress bar\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for i, (samples, targets, _) in p_bar:\n",
    "                x = i + num_batch * epoch  # number of iterations\n",
    "                if device == 'cuda':\n",
    "                    samples = samples.cuda().float() / 255\n",
    "                    targets = targets.cuda()\n",
    "                else:\n",
    "                    samples = samples.float() / 255\n",
    "                    targets = targets\n",
    "\n",
    "                # Warmup\n",
    "                if x <= num_warmup:\n",
    "                    xp = [0, num_warmup]\n",
    "                    fp = [1, 64 / batch_size]\n",
    "                    accumulate = max(1, np.interp(x, xp, fp).round())\n",
    "                    for j, y in enumerate(optimizer.param_groups):\n",
    "                        if j == 0:\n",
    "                            fp = [params['warmup_bias_lr'], y['initial_lr'] * lr(epoch)]\n",
    "                        else:\n",
    "                            fp = [0.0, y['initial_lr'] * lr(epoch)]\n",
    "                        y['lr'] = np.interp(x, xp, fp)\n",
    "                        if 'momentum' in y:\n",
    "                            fp = [params['warmup_momentum'], params['momentum']]\n",
    "                            y['momentum'] = np.interp(x, xp, fp)\n",
    "\n",
    "                # Forward\n",
    "                with torch.autocast(device, enabled=True):\n",
    "                    outputs = model(samples)  # forward\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                m_loss.update(loss.item(), samples.size(0))\n",
    "\n",
    "                loss *= batch_size  # loss scaled by batch_size\n",
    "        \n",
    "\n",
    "                # Backward\n",
    "                amp_scale.scale(loss).backward()\n",
    "\n",
    "                # Optimize\n",
    "                if x % accumulate == 0:\n",
    "                    amp_scale.unscale_(optimizer)  # unscale gradients\n",
    "                    clip_gradients(model)  # clip gradients\n",
    "                    amp_scale.step(optimizer)  # optimizer.step\n",
    "                    amp_scale.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    if ema:\n",
    "                        ema.update(model)\n",
    "\n",
    "                # Log\n",
    "                if local_rank == 0:\n",
    "                    if device == 'cuda':\n",
    "                        memory = f'{torch.cuda.memory_reserved() / 1E9:.3g}G'  # (GB)\n",
    "                    else:\n",
    "                        memory = f'{torch.cpu.memory_reserved() / 1E9:.3g}G'  # (GB)\n",
    "                    s = ('%10s' * 2 + '%10.4g') % (f'{epoch + 1}/{epochs}', memory, m_loss.avg)\n",
    "                    p_bar.set_description(s)\n",
    "\n",
    "                del loss\n",
    "                del outputs\n",
    "\n",
    "            # Scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            if local_rank == 0:\n",
    "                # mAP\n",
    "                last = test(dataset_path, device, input_size, params, ema.ema)\n",
    "                writer.writerow({'mAP': str(f'{last[1]:.3f}'),\n",
    "                                 'epoch': str(epoch + 1).zfill(3),\n",
    "                                 'mAP@50': str(f'{last[0]:.3f}')})\n",
    "                f.flush()\n",
    "\n",
    "                # Update best mAP\n",
    "                if last[1] > best:\n",
    "                    best = last[1]\n",
    "\n",
    "                # Save model\n",
    "                ckpt = {'model': copy.deepcopy(ema.ema).half()}\n",
    "\n",
    "                # Save last, best and delete\n",
    "                torch.save(ckpt, './weights/last.pt')\n",
    "                if best == last[1]:\n",
    "                    torch.save(ckpt, './weights/best.pt')\n",
    "                del ckpt\n",
    "\n",
    "    if local_rank == 0:\n",
    "        strip_optimizer('./weights/best.pt')  # strip optimizers\n",
    "        strip_optimizer('./weights/last.pt')  # strip optimizers\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08dfaa",
   "metadata": {},
   "source": [
    "### `test`\n",
    "- Evaluates the model's performance on a test dataset using a no-gradient context for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(dataset_path, device, input_size, params, model=None):\n",
    "\n",
    "    if device == 'cuda' or device == 'gpu':\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    filenames = []\n",
    "    \n",
    "    try:\n",
    "        with open(f'{dataset_path}/val2017.txt') as reader:\n",
    "            tmp_filenames = reader.readlines()\n",
    "    except:\n",
    "        tmp_filenames = os.listdir(f\"{dataset_path}/images/val\")\n",
    "    for filename in tmp_filenames:\n",
    "        filename = filename.rstrip().split('/')[-1]\n",
    "        filenames.append(f'{dataset_path}/images/val2017/' + filename)\n",
    "\n",
    "    dataset = Dataset(filenames, input_size, params, False)\n",
    "    loader = data.DataLoader(dataset, 8, False, num_workers=8,\n",
    "                             pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "\n",
    "    if model is None:\n",
    "        model = torch.load('./weights/best.pt', map_location=device)['model'].float()\n",
    "\n",
    "    model.half()\n",
    "    model.eval()\n",
    "\n",
    "    # Configure\n",
    "    iou_v = torch.linspace(0.5, 0.95, 10)  # iou vector for mAP@0.5:0.95\n",
    "    if device == 'cuda':\n",
    "        iou_v = iou_v.cuda()\n",
    "    n_iou = iou_v.numel()\n",
    "\n",
    "    m_pre = 0.\n",
    "    m_rec = 0.\n",
    "    map50 = 0.\n",
    "    mean_ap = 0.\n",
    "    metrics = []\n",
    "    p_bar = tqdm.tqdm(loader, desc=('%10s' * 3) % ('precision', 'recall', 'mAP'))\n",
    "    for samples, targets, shapes in p_bar:\n",
    "        if device == 'cuda':\n",
    "            samples = samples.cuda()\n",
    "            targets = targets.cuda()\n",
    "        samples = samples.half()  # uint8 to fp16/32\n",
    "        samples = samples / 255  # 0 - 255 to 0.0 - 1.0\n",
    "        _, _, height, width = samples.shape  # batch size, channels, height, width\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(samples)\n",
    "\n",
    "        # NMS\n",
    "        if device == 'cuda':\n",
    "            targets[:, 2:] *= torch.tensor((width, height, width, height)).cuda()  # to pixels\n",
    "        else:\n",
    "            targets[:, 2:] *= torch.tensor((width, height, width, height))\n",
    "        outputs = non_max_suppression(outputs, 0.001, 0.65)\n",
    "\n",
    "        # Metrics\n",
    "        for i, output in enumerate(outputs):\n",
    "            labels = targets[targets[:, 0] == i, 1:]\n",
    "            if device == 'cuda':\n",
    "                correct = torch.zeros(output.shape[0], n_iou, dtype=torch.bool).cuda()\n",
    "            else:\n",
    "                correct = torch.zeros(output.shape[0], n_iou, dtype=torch.bool)\n",
    "\n",
    "            if output.shape[0] == 0:\n",
    "                if labels.shape[0]:\n",
    "                    if device == 'cuda':\n",
    "                        _res = torch.zeros((3, 0)).cuda()\n",
    "                    else:\n",
    "                        _res = torch.zeros((3, 0))\n",
    "                    metrics.append((correct, *_res))\n",
    "                continue\n",
    "\n",
    "            detections = output.clone()\n",
    "            scale(detections[:, :4], samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
    "\n",
    "            # Evaluate\n",
    "            if labels.shape[0]:\n",
    "                tbox = labels[:, 1:5].clone()  # target boxes\n",
    "                tbox[:, 0] = labels[:, 1] - labels[:, 3] / 2  # top left x\n",
    "                tbox[:, 1] = labels[:, 2] - labels[:, 4] / 2  # top left y\n",
    "                tbox[:, 2] = labels[:, 1] + labels[:, 3] / 2  # bottom right x\n",
    "                tbox[:, 3] = labels[:, 2] + labels[:, 4] / 2  # bottom right y\n",
    "                scale(tbox, samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
    "\n",
    "                correct = np.zeros((detections.shape[0], iou_v.shape[0]))\n",
    "                correct = correct.astype(bool)\n",
    "\n",
    "                t_tensor = torch.cat((labels[:, 0:1], tbox), 1)\n",
    "                iou = box_iou(t_tensor[:, 1:], detections[:, :4])\n",
    "                correct_class = t_tensor[:, 0:1] == detections[:, 5]\n",
    "                for j in range(len(iou_v)):\n",
    "                    x = torch.where((iou >= iou_v[j]) & correct_class)\n",
    "                    if x[0].shape[0]:\n",
    "                        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1)\n",
    "                        matches = matches.cpu().numpy()\n",
    "                        if x[0].shape[0] > 1:\n",
    "                            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "                            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "                        correct[matches[:, 1].astype(int), j] = True\n",
    "                correct = torch.tensor(correct, dtype=torch.bool, device=iou_v.device)\n",
    "            metrics.append((correct, output[:, 4], output[:, 5], labels[:, 0]))\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = [torch.cat(x, 0).cpu().numpy() for x in zip(*metrics)]  # to numpy\n",
    "    if len(metrics) and metrics[0].any():\n",
    "        tp, fp, m_pre, m_rec, map50, mean_ap = compute_ap(*metrics)\n",
    "\n",
    "    # Print results\n",
    "    print('%10.3g' * 3 % (m_pre, m_rec, mean_ap))\n",
    "\n",
    "    # Return results\n",
    "    model.float()  # for training\n",
    "    return map50, mean_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b16d9",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 640\n",
    "batch_size = 16\n",
    "local_rank = 0\n",
    "epochs = 500\n",
    "num_workers = 8\n",
    "device = 'cuda'\n",
    "\n",
    "# Learning rate parameters\n",
    "lr_params = {\n",
    "    'lr0': 0.01,                    # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "    'lrf': 0.01,                    # final OneCycleLR learning rate (lr0 * lrf)\n",
    "    'momentum': 0.937,              # SGD momentum/Adam beta1\n",
    "    'weight_decay': 0.0005,         # optimizer weight decay 5e-4\n",
    "    'warmup_epochs': 3.0,           # warmup epochs\n",
    "    'warmup_momentum': 0.8,         # warmup initial momentum\n",
    "    'warmup_bias_lr': 0.1            # warmup initial bias lr\n",
    "}\n",
    "# Loss function parameters\n",
    "loss_fn_params = {\n",
    "    'box': 7.5,                     # box loss gain\n",
    "    'cls': 0.5,                     # classification loss gain\n",
    "    'dfl': 1.5                      # distribution focal loss gain\n",
    "}\n",
    "# Augmentation parameters\n",
    "augment_params = {\n",
    "    'hsv_h': 0.015,                 # image HSV-Hue augmentation (fraction)\n",
    "    'hsv_s': 0.7,                   # image HSV-Saturation augmentation (fraction)\n",
    "    'hsv_v': 0.4,                   # image HSV-Value augmentation (fraction)\n",
    "    'degrees': 0.0,                 # image rotation (+/- deg)\n",
    "    'translate': 0.1,               # image translation (+/- fraction)\n",
    "    'scale': 0.5,                   # image scale (+/- gain)\n",
    "    'shear': 0.0,                   # image shear (+/- deg)\n",
    "    'flip_ud': 0.0,                 # image flip up-down (probability)\n",
    "    'flip_lr': 0.5,                 # image flip left-right (probability)\n",
    "    'mosaic': 1.0,                  # image mosaic (probability)\n",
    "    'mix_up': 0.0                   # image mix-up (probability)\n",
    "}\n",
    "\n",
    "if not os.path.exists('weights'):\n",
    "    os.makedirs('weights')\n",
    "\n",
    "setup_seed()\n",
    "setup_multi_processes()\n",
    "\n",
    "with open('coco-class.yaml', errors='ignore') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "params.update(lr_params)\n",
    "params.update(loss_fn_params)\n",
    "params.update(augment_params)\n",
    "\n",
    "dataset_path = \"COCO/coco_minitrain_25k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e90b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataset_path, device, input_size, epochs, batch_size, num_workers, local_rank, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65698f48",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa197f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(dataset_path, device, input_size, params)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
