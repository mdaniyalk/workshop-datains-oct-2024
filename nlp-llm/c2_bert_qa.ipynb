{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOoUR08ESUJj"
      },
      "source": [
        "# Fine Tuning Bert Model on SQuAD 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (4.45.2)\n",
            "Requirement already satisfied: filelock in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from transformers) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-17 05:15:44--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M  1.18MB/s    in 39s     \n",
            "\n",
            "2024-10-17 05:16:26 (1.02 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n",
            "--2024-10-17 05:16:26--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.110.153, 185.199.111.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json’\n",
            "\n",
            "dev-v2.0.json       100%[===================>]   4.17M  2.23MB/s    in 1.9s    \n",
            "\n",
            "2024-10-17 05:16:28 (2.23 MB/s) - ‘dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O train-v2.0.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "!wget -O dev-v2.0.json https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-ruuo--TQVI",
        "outputId": "844ea17c-b55a-495c-8590-a64c121a1358"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mdaniyalk/miniforge3/envs/workshop/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForQuestionAnswering\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "\n",
        "train_file = 'train-v2.0.json'\n",
        "validation_file = 'dev-v2.0.json'\n",
        "\n",
        "with open(train_file) as f:\n",
        "    raw_train_data = json.load(f)\n",
        "with open(validation_file) as f:\n",
        "    raw_val_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampled_train_data = raw_train_data['data'][:200]\n",
        "raw_train_data['data'] = sampled_train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6A6hZwbKdTw"
      },
      "source": [
        "Install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBgnJihHN50K"
      },
      "source": [
        "Create the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "b80600d0c6e34498b6058c2043dcf1b5",
            "dd087c0a91ac44cbacae1ade1f7129dd",
            "4d068eeca50a464f99e2df03a1068711",
            "93a69cbdd7ec49c6b124a8860097621a",
            "11beb319c00e46aa86b300e22fd16f6e",
            "ed179a4fc3cb473db43d27fb82f48240",
            "87e65a0dc6de4b80a66f219dba8be394",
            "f72d3df4ed2841ad9fb5c70d20362c20"
          ]
        },
        "id": "uTmTzd0WzLCu",
        "outputId": "856b0251-1d06-4c76-8887-7285e83d2ac4"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "save_path = \"bert_base_uncased/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)\n",
        "max_len = 384\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CACrKo9MN9-T"
      },
      "source": [
        "Create a class to save the data form sQuAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "qyZR4BnGQKYY"
      },
      "outputs": [],
      "source": [
        "class SquadDataset:\n",
        "    def __init__(self, context, question, basic_answer, more_answers, start_idx):\n",
        "        self.context = context\n",
        "        self.question = question\n",
        "        self.basic_answer = basic_answer\n",
        "        self.more_answers = more_answers\n",
        "        self.start_idx = start_idx\n",
        "        self.end_idx = None\n",
        "        self.start_idx_token = start_idx\n",
        "        self.end_idx_token = None\n",
        "        self.offsets = None\n",
        "        self.input_ids = None \n",
        "        self.attention_mask = None\n",
        "        self.token_type_ids = None\n",
        "        self.validExample = True\n",
        "\n",
        "    def preprocess(self): # function to preprocess data\n",
        "       # Clean context, answer and question\n",
        "        self.context = \" \".join(str(self.context).split())\n",
        "        self.question = \" \".join(str(self.question).split())\n",
        "        \n",
        "        contextTokenizer =  tokenizer.encode(self.context)\n",
        "\n",
        "        if self.basic_answer is not None: # in case we have an answer\n",
        "\n",
        "          self.basic_answer = \" \".join(str(self.basic_answer).split())\n",
        "          #Calculate end_idx\n",
        "          self.end_idx = self.start_idx + len(self.basic_answer)\n",
        "          if (self.end_idx >=len(self.context)):\n",
        "              self.validExample= False\n",
        "              return\n",
        "\n",
        "          #find characters of context that are part of answer\n",
        "          is_part_of_answer = [0]*len(self.context)\n",
        "          for i in range (self.start_idx, self.end_idx):\n",
        "              is_part_of_answer[i] = 1\n",
        "\n",
        "          \n",
        "          #find index of token that corresponds to start and the end of the answer\n",
        "          answer_id_token=[]\n",
        "          for idx, (start,end) in enumerate(contextTokenizer.offsets):\n",
        "              if (sum(is_part_of_answer[start:end]) >0 ):\n",
        "                  answer_id_token.append(idx)\n",
        "          #data to predict\n",
        "          if len(answer_id_token) == 0 :\n",
        "              self.validExample=False\n",
        "              return         \n",
        "          self.start_idx_token = answer_id_token[0]\n",
        "          self.end_idx_token = answer_id_token[-1]\n",
        "        self.offsets = contextTokenizer.offsets\n",
        "        \n",
        "        # work on question\n",
        "        questionTokinizer  = tokenizer.encode(self.question)\n",
        "\n",
        "        #Create model's inputs \n",
        "        self.input_ids = contextTokenizer.ids + questionTokinizer.ids[1:]\n",
        "        self.attention_mask = [1] * len (self.input_ids)\n",
        "        self.token_type_ids = [0] * len(contextTokenizer.ids) + [1]*len(questionTokinizer.ids[1:])  \n",
        "\n",
        "        # fix padding\n",
        "        padding_length = max_len - len(self.input_ids)\n",
        "        if padding_length > 0:\n",
        "            self.input_ids = self.input_ids + ([0] * padding_length)\n",
        "            self.attention_mask = self.attention_mask + ([0] * padding_length)\n",
        "            self.token_type_ids = self.token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:\n",
        "            self.validExample= False \n",
        "            return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq0fUTFkOg39"
      },
      "source": [
        "Function that helps save data from json files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-kLAD7_muzhV"
      },
      "outputs": [],
      "source": [
        "def create_squad_examples(raw_data):\n",
        "    squad_examples = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                if qa[\"answers\"]:\n",
        "                  answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                  all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                  start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                  #context, question, basic_answer, more_answers, start_idx\n",
        "                  squad_eg = SquadDataset(context,question, answer_text, all_answers, start_char_idx)\n",
        "                else:\n",
        "                  squad_eg = SquadDataset(context,question, None, None, None) #context, question\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "    return squad_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nsI6692O6Jl"
      },
      "source": [
        "Function that create ttwo dictionaries, one for the input and one for the target "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MdfWynK3uz9F"
      },
      "outputs": [],
      "source": [
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\" : [],\n",
        "        \"attention_mask\" : [],\n",
        "        \"token_type_ids\" : [],\n",
        "        \"start_idx_token\" : [],\n",
        "        \"end_idx_token\" : []\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if item.validExample is True:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key],dtype=np.float16)\n",
        "    x = [dataset_dict[\"input_ids\"], dataset_dict[\"attention_mask\"], dataset_dict[\"token_type_ids\"]]\n",
        "    y = [dataset_dict[\"start_idx_token\"], dataset_dict[\"end_idx_token\"]]\n",
        "    return x, y   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ev1vzsRcU6Jx"
      },
      "outputs": [],
      "source": [
        "data = create_squad_examples(raw_train_data)  # save the data for the training set\n",
        "\n",
        "val_data = create_squad_examples(raw_val_data)  # save the data for the validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJud54kGPbZE"
      },
      "source": [
        "Check what is saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "ug8SwSkZ_uTB",
        "outputId": "721b5153-32eb-4f24-9e8f-d414ea93e6bb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>basic_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>in the late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>singing and dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In what city and state did Beyonce grow up?</td>\n",
              "      <td>Houston, Texas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  \\\n",
              "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "\n",
              "                                            question         basic_answer  \n",
              "0           When did Beyonce start becoming popular?    in the late 1990s  \n",
              "1  What areas did Beyonce compete in when she was...  singing and dancing  \n",
              "2  When did Beyonce leave Destiny's Child and bec...                 2003  \n",
              "3        In what city and state did Beyonce grow up?       Houston, Texas  \n",
              "4         In which decade did Beyonce become famous?           late 1990s  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = pd.DataFrame.from_records([vars(line) for line in data])\n",
        "train_data[[\"context\",\"question\",\"basic_answer\"]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "grzWupQh4BIk"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = create_inputs_targets(data)  # split the training data to input and target\n",
        "\n",
        "x_eval, y_eval = create_inputs_targets(val_data)  # split the validation data to input and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fThhkIjQk31B"
      },
      "outputs": [],
      "source": [
        "doc_stride = 64\n",
        "max_seq_length = 128\n",
        "max_query_length = 32\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFkXwvpOP0dX"
      },
      "source": [
        "### Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q8wpGNktunPQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert to Tensors and build dataset\n",
        "train_data = TensorDataset(torch.tensor(x_train[0], dtype=torch.int64),\n",
        "                           torch.tensor(x_train[1], dtype=torch.float),\n",
        "                           torch.tensor(x_train[2], dtype=torch.int64),\n",
        "                           torch.tensor(y_train[0], dtype=torch.int64),\n",
        "                           torch.tensor(y_train[1], dtype=torch.int64))\n",
        "\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s0bo33FHvcL3"
      },
      "outputs": [],
      "source": [
        "# Convert to Tensors and build dataset\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_data_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "eval_data = TensorDataset(torch.tensor(x_eval[0], dtype=torch.int64),\n",
        "                          torch.tensor(x_eval[1], dtype=torch.float),\n",
        "                          torch.tensor(x_eval[2], dtype=torch.int64),\n",
        "                          torch.tensor(y_eval[0], dtype=torch.int64),\n",
        "                          torch.tensor(y_eval[1], dtype=torch.int64))\n",
        "\n",
        "eval_sampler = SequentialSampler(eval_data)\n",
        "validation_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "752yxQ8PP8BL"
      },
      "source": [
        "### Check for GPU availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIt22mNknCAN",
        "outputId": "45bbaaae-fd31-4216-e560-270f6d86195e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hCGNA6qQCjV"
      },
      "source": [
        "### Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "0fa5c0384f904912907c6cea29fa1ac4",
            "663fc6642e3740acb0556b214a8ab9ce",
            "2c66f7c89f97464dbe8aa5b2352eeb0a",
            "c225cd2020da460aadd62a5675910f09",
            "451407260dd94051af3663d90004de7f",
            "a85a67e4ef2f4dc699c1ef6c39e99581",
            "4ab5ec4d07064505980dbbe5c7f123d4",
            "01094799a52a46f29f9504aa26ecbc84",
            "884c6223698a4205b0c5d896958def45",
            "ae26a72556df4a9594774492019b0a16",
            "5cd9c4e16b36485480e65fb533b7c495",
            "e228b477aeaf4ed69c2be350554c1eb2",
            "8b2bf6952a5645648eff3bc30739bc6c",
            "782e4892fdad47ec953208411fac3564",
            "1ccddc1238bb424d840ff705dccedf0f",
            "8abdfa0f416340c9b4d5b5e1968e7824"
          ]
        },
        "id": "rlsIqAKRsrLE",
        "outputId": "7d5fd80e-eb53-4a40-9d45-67c390756f01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased').to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kbfGuV8NtVkF"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQCQ__PIQPaL"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLT084mGQJEM"
      },
      "source": [
        "Function for text normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "g7n6yBDTCTZV"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    text = re.sub(regex, \" \", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjScNs3OuF0L",
        "outputId": "b6ec73af-aa5b-44cd-d909-c824314b15d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Steps:   0%|          | 4/3616 [04:04<61:18:32, 61.11s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m:       batch[\u001b[39m0\u001b[39m],\n\u001b[1;32m     15\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m:  batch[\u001b[39m1\u001b[39m], \n\u001b[1;32m     16\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m:  batch[\u001b[39m2\u001b[39m],  \n\u001b[1;32m     17\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mstart_positions\u001b[39m\u001b[39m'\u001b[39m: batch[\u001b[39m3\u001b[39m], \n\u001b[1;32m     18\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mend_positions\u001b[39m\u001b[39m'\u001b[39m:   batch[\u001b[39m4\u001b[39m]}\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     23\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1978\u001b[0m, in \u001b[0;36mBertForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1966\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1967\u001b[0m \u001b[39mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1968\u001b[0m \u001b[39m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1974\u001b[0m \u001b[39m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1975\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1976\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1978\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1979\u001b[0m     input_ids,\n\u001b[1;32m   1980\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1981\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1982\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1983\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1984\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1985\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1986\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1987\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1988\u001b[0m )\n\u001b[1;32m   1990\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1992\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqa_outputs(sequence_output)\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1143\u001b[0m     embedding_output,\n\u001b[1;32m   1144\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1145\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1146\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1147\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1148\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1149\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1150\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1151\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1152\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1153\u001b[0m )\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    696\u001b[0m         hidden_states,\n\u001b[1;32m    697\u001b[0m         attention_mask,\n\u001b[1;32m    698\u001b[0m         layer_head_mask,\n\u001b[1;32m    699\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    700\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    701\u001b[0m         past_key_value,\n\u001b[1;32m    702\u001b[0m         output_attentions,\n\u001b[1;32m    703\u001b[0m     )\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    586\u001b[0m         hidden_states,\n\u001b[1;32m    587\u001b[0m         attention_mask,\n\u001b[1;32m    588\u001b[0m         head_mask,\n\u001b[1;32m    589\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    590\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    591\u001b[0m     )\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    516\u001b[0m         hidden_states,\n\u001b[1;32m    517\u001b[0m         attention_mask,\n\u001b[1;32m    518\u001b[0m         head_mask,\n\u001b[1;32m    519\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    520\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    521\u001b[0m         past_key_value,\n\u001b[1;32m    522\u001b[0m         output_attentions,\n\u001b[1;32m    523\u001b[0m     )\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/workshop/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m is_causal \u001b[39m=\u001b[39m (\n\u001b[1;32m    437\u001b[0m     \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cross_attention \u001b[39mand\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m tgt_len \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    438\u001b[0m )\n\u001b[0;32m--> 440\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mscaled_dot_product_attention(\n\u001b[1;32m    441\u001b[0m     query_layer,\n\u001b[1;32m    442\u001b[0m     key_layer,\n\u001b[1;32m    443\u001b[0m     value_layer,\n\u001b[1;32m    444\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    445\u001b[0m     dropout_p\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_prob \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39melse\u001b[39;49;00m \u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m    446\u001b[0m     is_causal\u001b[39m=\u001b[39;49mis_causal,\n\u001b[1;32m    447\u001b[0m )\n\u001b[1;32m    449\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    450\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mreshape(bsz, tgt_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 1\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(\"Training epoch \", str(epoch))\n",
        "   \n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_steps = 0\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Training Steps\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        inputs = {'input_ids':       batch[0],\n",
        "                  'attention_mask':  batch[1], \n",
        "                  'token_type_ids':  batch[2],  \n",
        "                  'start_positions': batch[3], \n",
        "                  'end_positions':   batch[4]}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    print(f\"\\nTraining loss={tr_loss / nb_tr_steps:.4f}\")\n",
        "\n",
        "\n",
        "    # ============================================ VALIDATION ==========================================================\n",
        "    model.eval()\n",
        "    currect_query = 0\n",
        "    correct_ans = 0\n",
        "    valid_examples = [x for x in val_data if x.validExample is True]\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        input_ids, attention_mask, token_type_ids, start_positions, end_positions = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "          start_logits, end_logits =  model(input_ids=input_ids,\n",
        "                                              attention_mask=attention_mask,\n",
        "                                              token_type_ids=token_type_ids, return_dict=False)\n",
        "          \n",
        "          pred_start, pred_end = start_logits.detach().cpu().numpy(), end_logits.detach().cpu().numpy()\n",
        "\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "            squad_eg = valid_examples[currect_query]\n",
        "            currect_query += 1\n",
        "            offsets = squad_eg.offsets\n",
        "            start = np.argmax(start)\n",
        "            end = np.argmax(end)\n",
        "            if start >= len(offsets):\n",
        "                continue\n",
        "            pred_char_start = offsets[start][0]\n",
        "            if end < len(offsets):\n",
        "                pred_char_end = offsets[end][1]\n",
        "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                pred_ans = squad_eg.context[pred_char_start:]\n",
        "            normalized_pred_ans = normalize_text(pred_ans)\n",
        "            normalized_true_ans = [normalize_text(x) for x in squad_eg.more_answers]\n",
        "            if normalized_pred_ans in normalized_true_ans:\n",
        "                correct_ans += 1\n",
        "    acc = correct_ans / len(y_eval[0])\n",
        "\n",
        "    print(f\"\\nAccuracy score={acc:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqrNzdjFqg0I"
      },
      "source": [
        "### Testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4gYupzjXreU",
        "outputId": "39b8ad89-6008-420c-bdf1-b3d1aea654c3"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    \"data\": [\n",
        "        {\n",
        "            \"title\": \"Universitas Gadjah Mada\",\n",
        "            \"paragraphs\": [\n",
        "                {\n",
        "                    \"context\": \"Universitas Gadjah Mada (UGM) is a public research university located in \"\n",
        "                    \"Sleman, Special Region of Yogyakarta, Indonesia. Officially founded on 19 December 1949, \"\n",
        "                    \"Gadjah Mada University is one of the oldest and largest institutions of higher education \"\n",
        "                    \"in the country, and has been credited as one of the best universities in Indonesia. In the \"\n",
        "                    \"2024 QS World University Rankings, UGM is ranked 2nd in Indonesia and 263rd in the world. \"\n",
        "                    \"During the period when native education was often restricted, the institution was the first \"\n",
        "                    \"to open its medicine to native Indonesians when it was founded in the 1940s under Dutch rule. \"\n",
        "                    \"Comprising 18 faculties and 27 research centers, UGM offers 68 undergraduate, 23 diplomas, \"\n",
        "                    \"104 master's and specialist, 43 doctorates, and 4 clusters of post-doctoral study programs. \"\n",
        "                    \"The university has enrolled approximately 55,000 students, 1,187 foreign students, and \"\n",
        "                    \"has 2,500 faculty members. UGM maintains a campus of 360 acres (150 ha), with facilities \"\n",
        "                    \"that include a stadium and a fitness center. The university is named after Gajah Mada, \"\n",
        "                    \"the leader of the Majapahit Empire of Java in the 14th century, who is also considered \"\n",
        "                    \"to be the nation's first unifier by some historians; The spelling of the university's \"\n",
        "                    \"name still reflects the old Dutch-era spelling.\",\n",
        "                    \"qas\": [\n",
        "                        {\n",
        "                            \"question\": \"When was UGM officially founded?\",\n",
        "                            \"id\": \"Q1\",\n",
        "                            \"answers\": \"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"question\": \"What is the ranking of UGM in Indonesia according to the 2024 QS World University Rankings?\",\n",
        "                            \"id\": \"Q2\",\n",
        "                            \"answers\": \"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"question\": \"How many faculties does UGM have?\",\n",
        "                            \"id\": \"Q3\",\n",
        "                            \"answers\": \"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"question\": \"How many research centers are affiliated with UGM?\",\n",
        "                            \"id\": \"Q4\",\n",
        "                            \"answers\": \"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"question\": \"Approximately how many students are enrolled at UGM?\",\n",
        "                            \"id\": \"Q5\",\n",
        "                            \"answers\": \"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"question\": \"Who is UGM named after?\",\n",
        "                            \"id\": \"Q6\",\n",
        "                            \"answers\": \"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"question\": \"What year did UGM open its medicine to native Indonesians?\",\n",
        "                            \"id\": \"Q7\",\n",
        "                            \"answers\": \"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"question\": \"How large is UGM's campus?\",\n",
        "                            \"id\": \"Q8\",\n",
        "                            \"answers\": \"\"\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "model.eval()\n",
        "test_samples = create_squad_examples(data)\n",
        "x_test, _ = create_inputs_targets(test_samples)\n",
        "pred_start, pred_end = model(torch.tensor(x_test[0], dtype=torch.int64, device=device),\n",
        "                             torch.tensor(x_test[1], dtype=torch.float, device=device),\n",
        "                             torch.tensor(x_test[2], dtype=torch.int64, device=device), return_dict=False)\n",
        "pred_start, pred_end = pred_start.detach().cpu().numpy(), pred_end.detach().cpu().numpy()\n",
        "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "    test_sample = test_samples[idx]\n",
        "    offsets = test_sample.offsets\n",
        "    start = np.argmax(start)\n",
        "    end = np.argmax(end)\n",
        "    pred_ans = None\n",
        "    if start >= len(offsets):\n",
        "        continue\n",
        "    pred_char_start = offsets[start][0]\n",
        "    if end < len(offsets):\n",
        "        pred_ans = test_sample.context[pred_char_start:offsets[end][1]]\n",
        "    else:\n",
        "        pred_ans = test_sample.context[pred_char_start:]\n",
        "    print(\"Q: \" + test_sample.question)\n",
        "    print(\"A: \" + pred_ans)\n",
        "    print(\"----------------------------------------\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "Question_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "orig_nbformat": 2,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01094799a52a46f29f9504aa26ecbc84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fa5c0384f904912907c6cea29fa1ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c66f7c89f97464dbe8aa5b2352eeb0a",
              "IPY_MODEL_c225cd2020da460aadd62a5675910f09"
            ],
            "layout": "IPY_MODEL_663fc6642e3740acb0556b214a8ab9ce"
          }
        },
        "11beb319c00e46aa86b300e22fd16f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "1ccddc1238bb424d840ff705dccedf0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c66f7c89f97464dbe8aa5b2352eeb0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a85a67e4ef2f4dc699c1ef6c39e99581",
            "max": 433,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_451407260dd94051af3663d90004de7f",
            "value": 433
          }
        },
        "451407260dd94051af3663d90004de7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4ab5ec4d07064505980dbbe5c7f123d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d068eeca50a464f99e2df03a1068711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed179a4fc3cb473db43d27fb82f48240",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11beb319c00e46aa86b300e22fd16f6e",
            "value": 231508
          }
        },
        "5cd9c4e16b36485480e65fb533b7c495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_782e4892fdad47ec953208411fac3564",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b2bf6952a5645648eff3bc30739bc6c",
            "value": 440473133
          }
        },
        "663fc6642e3740acb0556b214a8ab9ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "782e4892fdad47ec953208411fac3564": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87e65a0dc6de4b80a66f219dba8be394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "884c6223698a4205b0c5d896958def45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5cd9c4e16b36485480e65fb533b7c495",
              "IPY_MODEL_e228b477aeaf4ed69c2be350554c1eb2"
            ],
            "layout": "IPY_MODEL_ae26a72556df4a9594774492019b0a16"
          }
        },
        "8abdfa0f416340c9b4d5b5e1968e7824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b2bf6952a5645648eff3bc30739bc6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "93a69cbdd7ec49c6b124a8860097621a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f72d3df4ed2841ad9fb5c70d20362c20",
            "placeholder": "​",
            "style": "IPY_MODEL_87e65a0dc6de4b80a66f219dba8be394",
            "value": " 232k/232k [00:00&lt;00:00, 308kB/s]"
          }
        },
        "a85a67e4ef2f4dc699c1ef6c39e99581": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae26a72556df4a9594774492019b0a16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b80600d0c6e34498b6058c2043dcf1b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d068eeca50a464f99e2df03a1068711",
              "IPY_MODEL_93a69cbdd7ec49c6b124a8860097621a"
            ],
            "layout": "IPY_MODEL_dd087c0a91ac44cbacae1ade1f7129dd"
          }
        },
        "c225cd2020da460aadd62a5675910f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01094799a52a46f29f9504aa26ecbc84",
            "placeholder": "​",
            "style": "IPY_MODEL_4ab5ec4d07064505980dbbe5c7f123d4",
            "value": " 433/433 [00:16&lt;00:00, 25.6B/s]"
          }
        },
        "dd087c0a91ac44cbacae1ade1f7129dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e228b477aeaf4ed69c2be350554c1eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8abdfa0f416340c9b4d5b5e1968e7824",
            "placeholder": "​",
            "style": "IPY_MODEL_1ccddc1238bb424d840ff705dccedf0f",
            "value": " 440M/440M [00:15&lt;00:00, 27.5MB/s]"
          }
        },
        "ed179a4fc3cb473db43d27fb82f48240": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f72d3df4ed2841ad9fb5c70d20362c20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
