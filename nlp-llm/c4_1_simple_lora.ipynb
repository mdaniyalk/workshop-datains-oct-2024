{"cells":[{"cell_type":"markdown","id":"9b7aa03f","metadata":{"id":"9b7aa03f"},"source":["# Simple LoRA Adapters"]},{"cell_type":"code","execution_count":1,"id":"8209bea0","metadata":{"lines_to_next_cell":2,"id":"8209bea0","executionInfo":{"status":"ok","timestamp":1729138633699,"user_tz":-420,"elapsed":9287,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}}},"outputs":[],"source":["import math\n","from functools import partial\n","\n","import torch\n","import torch.nn.utils.parametrize as parametrize\n","from torch import nn\n","\n","\n","\n","_ = torch.set_grad_enabled(False)"]},{"cell_type":"code","execution_count":6,"id":"c3139aea","metadata":{"id":"c3139aea","executionInfo":{"status":"ok","timestamp":1729139260013,"user_tz":-420,"elapsed":387,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}}},"outputs":[],"source":["class LoRAParametrization(nn.Module):\n","    def __init__(self, fan_in, fan_out, fan_in_fan_out=False, rank=4, lora_dropout_p=0.0, lora_alpha=1):\n","        super().__init__()\n","        # if weight is stored as (fan_out, fan_in), the memory layout of A & B follows (W + BA)x\n","        # otherwise, it's x(W + AB). This allows us to tie the weights between linear layers and embeddings\n","        self.swap = (lambda x: (x[1], x[0])) if fan_in_fan_out else (lambda x: x)\n","        self.lora_A = nn.Parameter(torch.zeros(self.swap((rank, fan_in))))\n","        self.lora_B = nn.Parameter(torch.zeros(self.swap((fan_out, rank))))\n","        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n","        self.lora_alpha, self.rank = lora_alpha, rank\n","        self.scaling = lora_alpha / rank\n","        self.lora_dropout = nn.Dropout(p=lora_dropout_p) if lora_dropout_p > 0 else lambda x: x\n","        self.dropout_fn = self._dropout if lora_dropout_p > 0 else lambda x: x\n","        self.register_buffer(\"lora_dropout_mask\", torch.ones(self.swap((1, fan_in)), dtype=self.lora_A.dtype))\n","        self.forward_fn = self.lora_forward\n","\n","    def _dropout(self, A):\n","        # to mimic the original implementation: A @ dropout(x), we do (A * dropout(ones)) @ x\n","        return A * self.lora_dropout(self.lora_dropout_mask)\n","\n","    def lora_forward(self, X):\n","        return X + torch.matmul(*self.swap((self.lora_B, self.dropout_fn(self.lora_A)))).view(X.shape) * self.scaling\n","\n","    def forward(self, X):\n","        return self.forward_fn(X)\n","\n","    def disable_lora(self):\n","        self.forward_fn = lambda x: x\n","\n","    def enable_lora(self):\n","        self.forward_fn = self.lora_forward\n","\n","    @classmethod\n","    def from_linear(cls, layer, rank=4, lora_dropout_p=0.0, lora_alpha=1):\n","        fan_out, fan_in = layer.weight.shape\n","        return cls(\n","            fan_in, fan_out, fan_in_fan_out=False, rank=rank, lora_dropout_p=lora_dropout_p, lora_alpha=lora_alpha\n","        )\n","\n","    @classmethod\n","    def from_conv2d(cls, layer, rank=4, lora_dropout_p=0.0, lora_alpha=1):\n","        fan_out, fan_in = layer.weight.view(layer.weight.shape[0], -1).shape\n","        return cls(\n","            fan_in, fan_out, fan_in_fan_out=False, rank=rank, lora_dropout_p=lora_dropout_p, lora_alpha=lora_alpha\n","        )\n","\n","    @classmethod\n","    def from_embedding(cls, layer, rank=4, lora_dropout_p=0.0, lora_alpha=1):\n","        fan_in, fan_out = layer.weight.shape\n","        return cls(\n","            fan_in, fan_out, fan_in_fan_out=True, rank=rank, lora_dropout_p=lora_dropout_p, lora_alpha=lora_alpha\n","        )\n","\n","\n","default_lora_config = {  # specify which layers to add lora to, by default only add to linear layers\n","    nn.Linear: {\n","        \"weight\": partial(LoRAParametrization.from_linear, rank=4),\n","    },\n","}\n","\n","\n","def apply_lora(layer, register=True, merge=False, lora_config=default_lora_config):\n","    \"\"\"add lora parametrization to a layer, designed to be used with model.apply\"\"\"\n","    if register:\n","        if type(layer) in lora_config:\n","            for attr_name, parametrization in lora_config[type(layer)].items():\n","                parametrize.register_parametrization(layer, attr_name, parametrization(layer))\n","    else:  # this will remove all parametrizations, use with caution\n","        if hasattr(layer, \"parametrizations\"):\n","            for attr_name in layer.parametrizations.keys():\n","                parametrize.remove_parametrizations(layer, attr_name, leave_parametrized=merge)\n","\n","\n","def add_lora(model, lora_config=default_lora_config):\n","    \"\"\"add lora parametrization to all layers in a model. Calling it twice will add lora twice\"\"\"\n","    model.apply(partial(apply_lora, lora_config=lora_config))\n","\n","\n","def add_lora_by_name(model, target_module_names, lora_config=default_lora_config):\n","    \"\"\"Add LoRA parameterization to specific layers in a model by names\"\"\"\n","    for name, layer in model.named_modules():\n","        if any([m in name for m in target_module_names]):\n","            add_lora(layer, lora_config=lora_config)\n","\n","\n","def merge_lora(model):\n","    \"\"\"merge lora parametrization to all layers in a model. This will remove all parametrization\"\"\"\n","    model.apply(partial(apply_lora, register=False, merge=True))\n","\n","\n","def remove_lora(model):\n","    \"\"\"remove lora parametrization to all layers in a model. This will remove all parametrization\"\"\"\n","    model.apply(partial(apply_lora, register=False, merge=False))"]},{"cell_type":"code","execution_count":5,"id":"e07c2ef7","metadata":{"id":"e07c2ef7","executionInfo":{"status":"ok","timestamp":1729139256925,"user_tz":-420,"elapsed":421,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}}},"outputs":[],"source":["def apply_to_lora(fn):\n","    \"\"\"apply a function to LoRAParametrization layers, designed to be used with model.apply\"\"\"\n","\n","    def apply_fn(layer):\n","        if isinstance(layer, LoRAParametrization):\n","            fn(layer)\n","\n","    return apply_fn\n","\n","\n","enable_lora = lambda model: model.apply(apply_to_lora(lambda x: x.enable_lora()))\n","disable_lora = lambda model: model.apply(apply_to_lora(lambda x: x.disable_lora()))\n","\n","\n","# ------------------- helper function for collecting parameters for training/saving -------------------\n","\n","\n","def name_is_lora(name):\n","    return (\n","        len(name.split(\".\")) >= 4\n","        and (name.split(\".\")[-4]) == \"parametrizations\"\n","        and name.split(\".\")[-1] in [\"lora_A\", \"lora_B\"]\n","    )\n","\n","\n","def name_is_bias(name):\n","    return name.split(\".\")[-1] == \"bias\"\n","\n","\n","def get_params_by_name(model, print_shapes=False, name_filter=None):\n","    for n, p in model.named_parameters():\n","        if name_filter is None or name_filter(n):\n","            if print_shapes:\n","                print(n, p.shape)\n","            yield p\n","\n","\n","def get_lora_params(model, print_shapes=False):\n","    return get_params_by_name(model, print_shapes=print_shapes, name_filter=name_is_lora)\n","\n","\n","def get_bias_params(model, print_shapes=False):\n","    return get_params_by_name(model, print_shapes=print_shapes, name_filter=name_is_bias)\n","\n","\n","def get_lora_state_dict(model):\n","    return {k: v for k, v in model.state_dict().items() if name_is_lora(k)}\n","\n","\n","# ------------------- helper function for inferencing with multiple lora -------------------\n","\n","\n","def _prepare_for_multiple_lora(lora_layer):\n","    lora_layer.lora_As = []\n","    lora_layer.lora_Bs = []\n","\n","\n","def _append_lora(lora_layer):\n","    lora_layer.lora_As.append(nn.Parameter(lora_layer.lora_A.clone()))\n","    lora_layer.lora_Bs.append(nn.Parameter(lora_layer.lora_B.clone()))\n","\n","\n","def load_multiple_lora(model, lora_state_dicts):\n","    model.apply(apply_to_lora(_prepare_for_multiple_lora))\n","    for state_dict in lora_state_dicts:\n","        _ = model.load_state_dict(state_dict, strict=False)\n","        model.apply(apply_to_lora(_append_lora))\n","    return model\n","\n","\n","def _select_lora(lora_layer, index):\n","    lora_layer.lora_A = lora_layer.lora_As[index]\n","    lora_layer.lora_B = lora_layer.lora_Bs[index]\n","\n","\n","def select_lora(model, index):\n","    model.apply(apply_to_lora(lambda x: _select_lora(x, index)))\n","    return model\n","\n","\n","# ------------------- helper function for tying and untieing weights -------------------\n","\n","\n","def tie_weights(linear: nn.Linear, embedding: nn.Embedding):\n","    \"\"\"tie the weights of the linear layer and the embedding layer both with the same lora\"\"\"\n","    # this line below is optional if the original is already tied\n","    embedding.parametrizations.weight.original = linear.parametrizations.weight.original\n","    embedding.parametrizations.weight[0].lora_A = linear.parametrizations.weight[0].lora_B\n","    embedding.parametrizations.weight[0].lora_B = linear.parametrizations.weight[0].lora_A\n","\n","\n","def untie_weights(linear: nn.Linear, embedding: nn.Embedding):\n","    \"\"\"untie the weights of the linear layer and the embedding layer\"\"\"\n","    embedding.parametrizations.weight.original = nn.Parameter(embedding.weight.original.clone())\n","    embedding.parametrizations.weight[0].lora_A = nn.Parameter(embedding.parametrizations.weight[0].lora_A.clone())\n","    embedding.parametrizations.weight[0].lora_B = nn.Parameter(embedding.parametrizations.weight[0].lora_B.clone())"]},{"cell_type":"code","execution_count":3,"id":"492093a9","metadata":{"lines_to_next_cell":0,"id":"492093a9","executionInfo":{"status":"ok","timestamp":1729139220458,"user_tz":-420,"elapsed":420,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}},"outputId":"1739a2cb-7835-40b6-cef5-118176bee76c","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.1013,  0.1730, -0.1425]])\n"]}],"source":["# a simple model\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(in_features=5, out_features=7),\n","    torch.nn.Linear(in_features=7, out_features=3),\n",")\n","\n","x = torch.randn(1, 5)\n","y = model(x)\n","print(y)\n","Y0 = y"]},{"cell_type":"code","execution_count":8,"id":"98584a8c","metadata":{"lines_to_next_cell":0,"id":"98584a8c","executionInfo":{"status":"ok","timestamp":1729139278733,"user_tz":-420,"elapsed":413,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"844bc73f-a428-4625-c620-20dfed630125"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1013,  0.1730, -0.1425]])"]},"metadata":{},"execution_count":8}],"source":["# add lora to the model\n","# becase B is initialized to 0, the output is the same as before\n","add_lora(model)\n","y = model(x)\n","assert torch.allclose(y, Y0)\n","y"]},{"cell_type":"code","execution_count":9,"id":"c0251891","metadata":{"lines_to_next_cell":0,"id":"c0251891","executionInfo":{"status":"ok","timestamp":1729139307564,"user_tz":-420,"elapsed":392,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}},"outputId":"082021a0-9581-4d7f-a9ee-7e9497fd9a8f","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.1691,  0.1404, -0.1350]])\n"]}],"source":["# to make the output different, we need to initialize B to something non-zero\n","model.apply(apply_to_lora(lambda x: torch.nn.init.ones_(x.lora_B)))\n","y = model(x)\n","print(y)\n","assert not torch.allclose(y, Y0)\n","Y1 = y"]},{"cell_type":"code","execution_count":11,"id":"196087bc","metadata":{"lines_to_next_cell":0,"id":"196087bc","executionInfo":{"status":"ok","timestamp":1729139321555,"user_tz":-420,"elapsed":609,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"30c83a7a-dd67-4a1a-d3ec-f8f31cc627fb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1013,  0.1730, -0.1425]])"]},"metadata":{},"execution_count":11}],"source":["# now let's try to disable lora, the output is the same as before lora is added\n","disable_lora(model)\n","y = model(x)\n","assert torch.allclose(y, Y0)\n","y"]},{"cell_type":"code","execution_count":12,"id":"1e9cba3c","metadata":{"lines_to_next_cell":0,"id":"1e9cba3c","executionInfo":{"status":"ok","timestamp":1729139336303,"user_tz":-420,"elapsed":3,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9fbba79a-bda5-4dab-e28d-7b7f1d129e30"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1691,  0.1404, -0.1350]])"]},"metadata":{},"execution_count":12}],"source":["# enable lora again\n","enable_lora(model)\n","y = model(x)\n","assert torch.allclose(y, Y1)\n","y"]},{"cell_type":"code","execution_count":13,"id":"57f19300","metadata":{"id":"57f19300","executionInfo":{"status":"ok","timestamp":1729139344238,"user_tz":-420,"elapsed":450,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}},"outputId":"9c558228-248c-44c1-8554-45fd1222f17b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['0.parametrizations.weight.0.lora_A', '0.parametrizations.weight.0.lora_B', '1.parametrizations.weight.0.lora_A', '1.parametrizations.weight.0.lora_B'])"]},"metadata":{},"execution_count":13}],"source":["# let's save the state dict for later use\n","state_dict_to_save = get_lora_state_dict(model)\n","state_dict_to_save.keys()"]},{"cell_type":"code","execution_count":14,"id":"19a06b21","metadata":{"id":"19a06b21","executionInfo":{"status":"ok","timestamp":1729139350336,"user_tz":-420,"elapsed":493,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}}},"outputs":[],"source":["# you can remove lora from the model\n","remove_lora(model)"]},{"cell_type":"code","execution_count":15,"id":"522e71f1","metadata":{"lines_to_next_cell":0,"id":"522e71f1","executionInfo":{"status":"ok","timestamp":1729139360380,"user_tz":-420,"elapsed":410,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5ae4f9df-33d5-4d13-f2e1-4a78fb2351a8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1691,  0.1404, -0.1350]])"]},"metadata":{},"execution_count":15}],"source":["# lets try to load the lora back\n","# first we need to add lora to the model\n","add_lora(model)\n","# then we can load the lora parameters\n","# strict=False is needed because we are loading a subset of the parameters\n","_ = model.load_state_dict(state_dict_to_save, strict=False)\n","y = model(x)\n","assert torch.allclose(y, Y1)\n","y"]},{"cell_type":"code","execution_count":16,"id":"9f0c8570","metadata":{"lines_to_next_cell":0,"id":"9f0c8570","executionInfo":{"status":"ok","timestamp":1729139368072,"user_tz":-420,"elapsed":422,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0c70deb-7285-4515-ad7f-03311be7dffe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1691,  0.1404, -0.1350]])"]},"metadata":{},"execution_count":16}],"source":["# we can merge it to make it a normal linear layer, so there is no overhead for inference\n","merge_lora(model)\n","y = model(x)\n","assert torch.allclose(y, Y1)\n","y"]},{"cell_type":"code","execution_count":null,"id":"ee283143","metadata":{"id":"ee283143","executionInfo":{"status":"ok","timestamp":1729127926121,"user_tz":-420,"elapsed":2,"user":{"displayName":"Daniyal Kautsar","userId":"16826653404453784158"}},"outputId":"ca76d955-43aa-45bd-f735-6e960946f9d9","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=5, out_features=7, bias=True)\n","  (1): Linear(in_features=7, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":13}],"source":["# model now has no lora parameters\n","model"]},{"cell_type":"markdown","id":"f3c246e1","metadata":{"id":"f3c246e1"},"source":["## Training a model"]},{"cell_type":"code","execution_count":17,"id":"edfaee1e","metadata":{"lines_to_next_cell":0,"id":"edfaee1e","executionInfo":{"status":"ok","timestamp":1729139387530,"user_tz":-420,"elapsed":5552,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}}},"outputs":[],"source":["model = torch.nn.Linear(in_features=5, out_features=3)\n","# Step 1: Add LoRA to the model\n","add_lora(model)\n","\n","# Step 2: Collect the parameters, pass them to the optimizer\n","\n","parameters = [\n","    {\"params\": list(get_lora_params(model))},\n","]\n","optimizer = torch.optim.AdamW(parameters, lr=1e-3)\n","\n","# Step 3: Train the model\n","# ...\n","# simulate training, update the LoRA parameters\n","model.apply(apply_to_lora(lambda x: torch.nn.init.normal_(x.lora_A)))\n","model.apply(apply_to_lora(lambda x: torch.nn.init.normal_(x.lora_B)))\n","\n","# Step 4: export the LoRA parameters\n","state_dict = model.state_dict()\n","lora_state_dict = {k: v for k, v in state_dict.items() if name_is_lora(k)}"]},{"cell_type":"markdown","id":"539e7d19","metadata":{"id":"539e7d19"},"source":["## Loading and Inferencing with LoRA"]},{"cell_type":"code","execution_count":19,"id":"1a9836de","metadata":{"id":"1a9836de","executionInfo":{"status":"ok","timestamp":1729139413013,"user_tz":-420,"elapsed":419,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a6955db4-334c-47cb-bb4f-c6c9b5c4138b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.3719, -1.0054, -1.6655]])"]},"metadata":{},"execution_count":19}],"source":["# Step 1: Add LoRA to your model\n","add_lora(model)\n","\n","# Step 2: Load the LoRA parameters\n","_ = model.load_state_dict(lora_state_dict, strict=False)\n","\n","# Step 3: Merge the LoRA parameters into the model\n","merge_lora(model)\n","\n","y = model(x)\n","y"]},{"cell_type":"markdown","id":"ccba9d68","metadata":{"id":"ccba9d68"},"source":["## Inferencing with multiple LoRA models"]},{"cell_type":"code","execution_count":20,"id":"a0ef4b28","metadata":{"id":"a0ef4b28","executionInfo":{"status":"ok","timestamp":1729139422703,"user_tz":-420,"elapsed":546,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}}},"outputs":[],"source":["# to avoid re-adding lora to the model when rerun the cell, remove lora first\n","remove_lora(model)\n","# Step 1: Add LoRA to your model\n","add_lora(model)\n","\n","# Step 2: Load the LoRA parameters\n","\n","# fake 3 sets of LoRA parameters\n","lora_state_dict_0 = lora_state_dict\n","lora_state_dict_1 = {k: torch.ones_like(v) for k, v in lora_state_dict.items()}\n","lora_state_dict_2 = {k: torch.zeros_like(v) for k, v in lora_state_dict.items()}\n","lora_state_dicts = [lora_state_dict_0, lora_state_dict_1, lora_state_dict_2]\n","\n","load_multiple_lora(model, lora_state_dicts)\n","\n","# Step 3: Select which LoRA to use at inference time\n","Y0 = select_lora(model, 0)(x)\n","Y1 = select_lora(model, 1)(x)\n","Y2 = select_lora(model, 2)(x)"]},{"cell_type":"code","execution_count":21,"id":"c67602a3","metadata":{"id":"c67602a3","executionInfo":{"status":"ok","timestamp":1729139427756,"user_tz":-420,"elapsed":367,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}},"outputId":"3d26818a-d612-4fcc-ee31-7e5cd9f35951","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[-0.2349, -1.3566, -2.5691]]),\n"," tensor([[ 0.9081,  0.2746, -0.3855]]),\n"," tensor([[-0.3719, -1.0054, -1.6655]]))"]},"metadata":{},"execution_count":21}],"source":["Y0, Y1, Y2"]},{"cell_type":"code","execution_count":22,"id":"537c5c6c","metadata":{"id":"537c5c6c","executionInfo":{"status":"ok","timestamp":1729139436502,"user_tz":-420,"elapsed":360,"user":{"displayName":"Daniyal Kautsar","userId":"00481734402985491669"}},"outputId":"a7b25c2e-67bd-4f26-a4b0-0ed55a2f3968","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.2349, -1.3566, -2.5691]])\n","tensor([[ 0.9081,  0.2746, -0.3855]])\n","tensor([[-0.3719, -1.0054, -1.6655]])\n"]}],"source":["remove_lora(model)\n","init_state_dict = model.state_dict()\n","# verify that it's the same as if we load the lora parameters one by one\n","for state_dict in lora_state_dicts:\n","    remove_lora(model)\n","    _ = model.load_state_dict(init_state_dict, strict=False)\n","    add_lora(model)\n","    _ = model.load_state_dict(state_dict, strict=False)\n","    merge_lora(model)\n","    y = model(x)\n","    print(y)"]}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"lora","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"cd38cab5b092fbce1866c43acaed152c77b80a12cd5e2b7fb23112c1a171e061"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}